[{"path":"index.html","id":"about","chapter":"About","heading":"About","text":"這本 tutorial，是想把我熟悉的 R 語言技巧，轉換到 Python來。並著重在資料分析的領域(還不談ML)。所以就是最無聊但也最常用的幾個 part:\n資料讀取：從 csv, excel; 從 DB; 從 API; …\n資料轉換：tidyr, dplyr, stringr, lubridate, purrr 這些我很熟的 R 套件，轉換到 python 上\n資料視覺化： ggplot, plotly 要轉換到 python\n文件： RMarkdown 看如何轉換成 Jupyter Notebook\n網頁： shiny 轉換成 Dash Flask\nAPI: plumber 轉換成 Flask\n資料讀取：從 csv, excel; 從 DB; 從 API; …資料轉換：tidyr, dplyr, stringr, lubridate, purrr 這些我很熟的 R 套件，轉換到 python 上資料視覺化： ggplot, plotly 要轉換到 python文件： RMarkdown 看如何轉換成 Jupyter Notebook網頁： shiny 轉換成 Dash FlaskAPI: plumber 轉換成 Flask","code":""},{"path":"write-and-read-files.html","id":"write-and-read-files","chapter":"1 Write and read files","heading":"1 Write and read files","text":"","code":""},{"path":"write-and-read-files.html","id":"三部曲架構介紹","chapter":"1 Write and read files","heading":"1.1 三部曲架構介紹","text":"在python中，不管是讀or寫file，不像R都是一個function搞定，而是都要以三部曲來完成：\nf = open(): 打開一個file，命名為f\nf.write() / f.read(): 讀取或寫入\nf.close(): 關閉這個file\n在python中，不管是讀or寫file，不像R都是一個function搞定，而是都要以三部曲來完成：f = open(): 打開一個file，命名為ff.write() / f.read(): 讀取或寫入f.close(): 關閉這個file那首先要介紹的，就是open這個函數怎麼用。我們可以在console中打”?open”，或是在VS code 中打入 “open”，然後按住command/ctrl後，以滑鼠點擊open，就可以看到文檔那首先要介紹的，就是open這個函數怎麼用。我們可以在console中打”?open”，或是在VS code 中打入 “open”，然後按住command/ctrl後，以滑鼠點擊open，就可以看到文檔從文檔中，可以看到open的第一個argument是file，這就是他的檔案名稱; 第二個argument為mode，可key入以下選擇：\n‘r’: 打開舊file並讀取(default)\n‘w’: 打開舊file，寫檔案，並覆蓋過原舊file\n‘x’: 打開新file，寫檔案\n‘’: 打開舊file，接續在之前的最後面，寫檔案(= append 的意思)\n‘b’: binary mode，二元模式，會搭配前面的r/w/x/a來使用，例如mode = ‘wb’，就是以二進位來寫入\n‘t’: text mode(default)，就是以一般文字來寫入，會搭配前面的r/w/x/a來使用，例如mode = ‘wt’，就是以文字來寫入\n從文檔中，可以看到open的第一個argument是file，這就是他的檔案名稱; 第二個argument為mode，可key入以下選擇：‘r’: 打開舊file並讀取(default)‘w’: 打開舊file，寫檔案，並覆蓋過原舊file‘x’: 打開新file，寫檔案‘’: 打開舊file，接續在之前的最後面，寫檔案(= append 的意思)‘b’: binary mode，二元模式，會搭配前面的r/w/x/a來使用，例如mode = ‘wb’，就是以二進位來寫入‘t’: text mode(default)，就是以一般文字來寫入，會搭配前面的r/w/x/a來使用，例如mode = ‘wt’，就是以文字來寫入接下來，就先demo寫入的幾個mode: ‘x’, ‘’, 和’w’接下來，就先demo寫入的幾個mode: ‘x’, ‘’, 和’w’","code":""},{"path":"write-and-read-files.html","id":"file的寫入","chapter":"1 Write and read files","heading":"1.2 file的寫入","text":"","code":""},{"path":"write-and-read-files.html","id":"mode-x","chapter":"1 Write and read files","heading":"1.2.1 mode = ‘x’","text":"首先，我們想寫一個新的file，所以要用mode = ‘x’，作法如下就可以看到新增了一個test1.txt檔案，打開來看可以看到裡面就寫著line1就可以看到新增了一個test1.txt檔案，打開來看可以看到裡面就寫著line1接著，我們除了用f.write()外，也可以用f.writelines()，那這時的input就要是個iterable物件(e.g. list)，他就會去iterate裡面的element，一個一個寫入這樣，比如：接著，我們除了用f.write()外，也可以用f.writelines()，那這時的input就要是個iterable物件(e.g. list)，他就會去iterate裡面的element，一個一個寫入這樣，比如：寫出來的文件長這樣可以發現全部黏在一起了如果要換行，就加上”“就好結果就會變成","code":"f = open(file = 'data/test1.txt', mode = 'x')\nf.write(\"line1\")\nf.close()f = open(\"data/test2.txt\", mode = \"x\")\nf.writelines([\"I\", \"am\", \"tom-hanks\"])\nf.close()Iamtom-hanksf = open(\"data/test3.txt\", mode = \"x\")\nf.writelines([\"I\\n\", \"am\\n\", \"tom-hanks\\n\"])\nf.close()I\nam\ntom-hanks"},{"path":"write-and-read-files.html","id":"mode-a","chapter":"1 Write and read files","heading":"1.2.2 mode = ‘a’","text":"現在，我們已經有三個檔案了(test1.txt, test2.txt, test3.txt)，如果，我繼續用mode = ‘x’ 來寫入已存在的檔案，就會報error所以，對於已經存在的檔案，我們只能選擇mode = ‘w’來覆蓋過去，或是用mode = ’’ 來append新結果所以，對於已經存在的檔案，我們只能選擇mode = ‘w’來覆蓋過去，或是用mode = ’’ 來append新結果先來試試mode = ‘’先來試試mode = ‘’結果如下","code":"f = open('data/test3.txt', mode = 'x')\nf.write(\"just try it\")\nf.close()f = open('data/test3.txt', mode = 'a')\nf.write(\"this is new line\")\nf.close()I\nam\ntom-hanks\nthis is new line"},{"path":"write-and-read-files.html","id":"mode-w","chapter":"1 Write and read files","heading":"1.2.3 mode = ‘w’","text":"如果以mode = ’w’來寫的話，他會把之前的內容全部清空，然後寫入新內容結果就變成然後，’w’其實是最常用的mode，因為他也可以當’x’來用。就是說，如果你原本沒有test4.txt這個檔案，那你用mode = ’w’時，他會像mode = ’x’一樣，幫你新增這個檔案，然後寫進去","code":"f = open('data/test3.txt', mode = 'w')\nf.write(\"this is another line\")\nf.close()this is another linef = open('data/test4.txt', mode = 'w')\nf.write(\"Just like mode = 'x'\")\nf.close()"},{"path":"write-and-read-files.html","id":"寫中文記得加上encoding-utf8","chapter":"1 Write and read files","heading":"1.2.4 寫中文，記得加上encoding = ‘utf8’","text":"剛剛都是寫入英文文件，如果今天寫中文的話，照預設來走，會報錯(因為預設的編碼方式是’ascii’)，所以要加上encoding = ‘utf8’ (但我用mac寫的時候，不寫encoding也沒報錯，所以應該預設就是utf8)","code":"f = open('data/test_chinese.txt', mode = 'w', encoding = 'utf8')\nf.write(\"寫中文行不行？\")\nf.close"},{"path":"write-and-read-files.html","id":"file的讀取","chapter":"1 Write and read files","heading":"1.3 file的讀取","text":"讀取的話，就簡單多了，用mode = ’r’就好而且，mode = ‘r’ 是預設，所以甚至可以不用寫","code":"f = open('data/test4.txt')\nf.read()\n#> \"Just like mode = 'x'\"\nf.close()"},{"path":"flat-files.html","id":"flat-files","chapter":"2 Flat Files","heading":"2 Flat Files","text":"","code":""},{"path":"flat-files.html","id":"csv","chapter":"2 Flat Files","heading":"2.1 CSV","text":"flat files的代表就是csv，他的特色就是資料以plain text的方式做儲存(所以不會有color, bold,等格式)資料是用delimiter做分隔，像csv就是用逗號做分隔(sep = \",\")，其他還可以用tab鍵做分隔(sep = \"\\t\")、空白鍵做分隔(sep = \" \")、分號做分隔(sep = \";\")等但無論如何，你在pandas裡面，都是用pd.read_csv(data, sep = {delimiter})做讀取：","code":"\nlibrary(reticulate)import pandas as pd\nimport matplotlib.pyplot as plt# Read the CSV and assign it to the variable data\ntax_data = pd.read_csv(\"data/vt_tax_data_2016.csv\")\n\n# View the first few lines of data\nprint(tax_data.head())\n#>    STATEFIPS STATE  zipcode  agi_stub  ...  N11901  A11901  N11902  A11902\n#> 0         50    VT        0         1  ...   10820    9734   88260  138337\n#> 1         50    VT        0         2  ...   12820   20029   68760  151729\n#> 2         50    VT        0         3  ...   10810   24499   34600   90583\n#> 3         50    VT        0         4  ...    7320   21573   21300   67045\n#> 4         50    VT        0         5  ...   12500   67761   23320  103034\n#> \n#> [5 rows x 147 columns]print(tax_data.shape)\n#> (1476, 147)"},{"path":"flat-files.html","id":"modifying-flat-file-imports","chapter":"2 Flat Files","heading":"2.1.1 Modifying flat file imports","text":"這邊要講的是一些細節，尤其是在原本的csv檔案太大(例如100多萬個row)時，要如何處理\n選取特定的column就好(usecols = {column_name column_index list})\n選取前n個rows就好(nrows = {n})\n跳過你指定的rows(skiprows = [0, 2, 3]會跳過第0,2,3個row，skiprows = 50會跳過0~49個row)\n這邊要講的是一些細節，尤其是在原本的csv檔案太大(例如100多萬個row)時，要如何處理選取特定的column就好(usecols = {column_name column_index list})選取前n個rows就好(nrows = {n})跳過你指定的rows(skiprows = [0, 2, 3]會跳過第0,2,3個row，skiprows = 50會跳過0~49個row)","code":""},{"path":"flat-files.html","id":"選取特定column","chapter":"2 Flat Files","heading":"2.1.1.1 選取特定column","text":"原本的csv有147個column，太多了，很多我用不到，現在我只想load以下的column\nzipcode\nagi_stub (income group)\nmars1 (number single households)\nMARS2 (number households filing married)\nNUMDEP (number dependents)\n原本的csv有147個column，太多了，很多我用不到，現在我只想load以下的columnzipcodeagi_stub (income group)mars1 (number single households)MARS2 (number households filing married)NUMDEP (number dependents)","code":"# Create list of columns to use\ncols = [\"zipcode\", \"agi_stub\", \"mars1\", \"MARS2\", \"NUMDEP\"]\n\n# Create data frame from csv using only selected columns\ntax_subcol = pd.read_csv(\"data/vt_tax_data_2016.csv\", usecols = cols)\n\n# View counts of dependents and tax returns by income level\nprint(tax_subcol.groupby(\"agi_stub\").sum())\n#>           zipcode   mars1  MARS2  NUMDEP\n#> agi_stub                                \n#> 1         1439444  170320  28480   52490\n#> 2         1439444  104000  37690   64660\n#> 3         1439444   39160  45390   47330\n#> 4         1439444   11670  44410   37760\n#> 5         1439444    7820  67750   60730\n#> 6         1439444    1210  16340   16300"},{"path":"flat-files.html","id":"選取前n個rows-跳過某些row","chapter":"2 Flat Files","heading":"2.1.1.2 選取前n個rows, 跳過某些row","text":"現在，我們先讀取前500個rows，並存成vt_data_first500現在我想load之後的500個rows，命名成vt_data_next500現在我想load之後的500個rows，命名成vt_data_next500要做到這樣，需要以下幾個步驟：\n用skiprows = 500來跳過前500個rows\n用nrows = 500來說明我要取接下來這500個rows\n用header = None來說明從row = 501開始讀的row，他不是header，他就是row，你要直接讀取他\n用names = list(vt_data_first500)，來幫資料的欄位命名。這邊要注意到，list(vt_data_first500)就可以抓出vt_data_first500的colnames了\n要做到這樣，需要以下幾個步驟：用skiprows = 500來跳過前500個rows用nrows = 500來說明我要取接下來這500個rows用header = None來說明從row = 501開始讀的row，他不是header，他就是row，你要直接讀取他用names = list(vt_data_first500)，來幫資料的欄位命名。這邊要注意到，list(vt_data_first500)就可以抓出vt_data_first500的colnames了","code":"vt_data_first500 = pd.read_csv(\"data/vt_tax_data_2016.csv\", nrows = 500)\nprint(vt_data_first500.shape)\n#> (500, 147)vt_data_next500 = pd.read_csv(\"data/vt_tax_data_2016.csv\", \n                              nrows=500,\n                              skiprows=500,\n                              header=None,\n                              names=list(vt_data_first500))# View the Vermont data frames to confirm they're different\nprint(vt_data_first500.head())\n#>    STATEFIPS STATE  zipcode  agi_stub  ...  N11901  A11901  N11902  A11902\n#> 0         50    VT        0         1  ...   10820    9734   88260  138337\n#> 1         50    VT        0         2  ...   12820   20029   68760  151729\n#> 2         50    VT        0         3  ...   10810   24499   34600   90583\n#> 3         50    VT        0         4  ...    7320   21573   21300   67045\n#> 4         50    VT        0         5  ...   12500   67761   23320  103034\n#> \n#> [5 rows x 147 columns]\nprint(vt_data_next500.head())\n#>    STATEFIPS STATE  zipcode  agi_stub  ...  N11901  A11901  N11902  A11902\n#> 0         50    VT     5356         2  ...      50      76     130     212\n#> 1         50    VT     5356         3  ...      40     142      50     148\n#> 2         50    VT     5356         4  ...       0       0      30      87\n#> 3         50    VT     5356         5  ...      30     531      30     246\n#> 4         50    VT     5356         6  ...       0       0       0       0\n#> \n#> [5 rows x 147 columns]"},{"path":"flat-files.html","id":"handling-errors-and-missing-data","chapter":"2 Flat Files","heading":"2.1.2 Handling errors and missing data","text":"讀取Flat File，常碰到以下問題：\nColumn的data type錯誤 -> 可以用dtype = {column_name: type}來修正錯誤\n遺漏值的預設值和你想的不同(e.g. 某個欄位的數值=999代表missing value) -> 使用na_values argument來做\n直接丟error給你，無法parse成dataframe -> 看error message來解決問題\n讀取Flat File，常碰到以下問題：Column的data type錯誤 -> 可以用dtype = {column_name: type}來修正錯誤遺漏值的預設值和你想的不同(e.g. 某個欄位的數值=999代表missing value) -> 使用na_values argument來做直接丟error給你，無法parse成dataframe -> 看error message來解決問題","code":""},{"path":"flat-files.html","id":"specify-data-types","chapter":"2 Flat Files","heading":"2.1.2.1 Specify data types","text":"舉例來說，看一下剛剛的vt_data_first500的各欄位data type可以看到，zipcode被判斷為int64，但其實他是字串。而agi_stub是社經地位，為categorical variable，所以應該是”category”那我們就特別assign這兩個欄位的data type:nice，搞定。其實，更常見的workflow，是先把資料load下來，探索後，才轉換欄位的資料格式，而不是像現在這樣讀取的時後就轉。不過沒關係，這已經牽扯到資料清理的範圍，我們等資料清理那邊再來處理就好","code":"print(vt_data_first500.dtypes)\n#> STATEFIPS     int64\n#> STATE        object\n#> zipcode       int64\n#> agi_stub      int64\n#> N1            int64\n#>               ...  \n#> A85300        int64\n#> N11901        int64\n#> A11901        int64\n#> N11902        int64\n#> A11902        int64\n#> Length: 147, dtype: object# Create dict specifying data types for agi_stub and zipcode\ndata_types = {\"agi_stub\": \"category\",\n              \"zipcode\": \"str\"}\n\n# Load csv using dtype to set correct data types\ndata = pd.read_csv(\"data/vt_tax_data_2016.csv\", dtype = data_types)\n\n# Print data types of resulting frame\nprint(data.dtypes.head())\n#> STATEFIPS       int64\n#> STATE          object\n#> zipcode        object\n#> agi_stub     category\n#> N1              int64\n#> dtype: object"},{"path":"flat-files.html","id":"set-custom-na-values","chapter":"2 Flat Files","heading":"2.1.2.2 Set custom NA values","text":"這邊我們想做一個設定，把zipcode這個欄位中，數值=0的，都設為NA。作法如下：","code":"# Create dict specifying that 0s in zipcode are NA values\nnull_values = {\"zipcode\": 0}\n\n# Load csv using na_values keyword argument\ndata = pd.read_csv(\"data/vt_tax_data_2016.csv\", \n                   na_values = null_values)\n\n# View rows with NA ZIP codes\nprint(data[data.zipcode.isna()])\n#>    STATEFIPS STATE  zipcode  agi_stub  ...  N11901  A11901  N11902  A11902\n#> 0         50    VT      NaN         1  ...   10820    9734   88260  138337\n#> 1         50    VT      NaN         2  ...   12820   20029   68760  151729\n#> 2         50    VT      NaN         3  ...   10810   24499   34600   90583\n#> 3         50    VT      NaN         4  ...    7320   21573   21300   67045\n#> 4         50    VT      NaN         5  ...   12500   67761   23320  103034\n#> 5         50    VT      NaN         6  ...    3900   93123    2870   39425\n#> \n#> [6 rows x 147 columns]"},{"path":"flat-files.html","id":"skip-bad-data","chapter":"2 Flat Files","heading":"2.1.2.3 Skip bad data","text":"這邊舉了一個你pd.read_csv()會失敗的例子，就是原始資料中，有一個row，他的欄位數目比別人多那這樣在讀取的時候，就會無法parse成大家都一樣的欄位數，所以會error遇到這種問題，我們會稱剛剛那個row叫”bad line”，我們可以下參數，讓pandas跳過這個line，然後warning我們就好看code就懂了：","code":"try:\n  # Set warn_bad_lines to issue warnings about bad records\n  data = pd.read_csv(\"data/vt_tax_data_2016_corrupt.csv\", \n                     error_bad_lines = False, \n                     warn_bad_lines = True)\n  \n  # View first 5 records\n  print(data.head())\n  \nexcept pd.io.common.CParserError:\n    print(\"Your data contained rows that could not be parsed.\")"},{"path":"flat-files.html","id":"excel","chapter":"2 Flat Files","heading":"2.2 Excel","text":"","code":""},{"path":"flat-files.html","id":"single-worksheet","chapter":"2 Flat Files","heading":"2.2.1 Single worksheet","text":"只要用pd.read_excel()就可以讀excel了，超easy的只要用pd.read_excel()就可以讀excel了，超easy的一些重要的argument，介紹一下：\nnrows = 100，只取前100個row\nskiprows = 100，跳過前100個row\nskiprows = [1,2,3]，跳過這三個row\nusecols = [\"\",\"b\",\"c\"] 只讀colname= , b, c的column\nusecols = [0,1,2] 只讀這三個column\nusecols = \":C, E\"，這就是excel專屬的了，就是你看到的欄位A,B,C,E要讀進來\n一些重要的argument，介紹一下：nrows = 100，只取前100個rowskiprows = 100，跳過前100個rowskiprows = [1,2,3]，跳過這三個rowusecols = [\"\",\"b\",\"c\"] 只讀colname= , b, c的columnusecols = [0,1,2] 只讀這三個columnusecols = \":C, E\"，這就是excel專屬的了，就是你看到的欄位A,B,C,E要讀進來先來讀資料吧：先來讀資料吧：會發現，第一列根本就不是變數名稱，回去看一下excel原始檔，會發現，真正的變數是從第三列開始所以，我們用skiprows = 2來跳過前兩列接下來，試試讀部分資料： 跳過前兩個row，只選欄位AD, AW~BA的column:","code":"# Read spreadsheet and assign it to survey_responses\nsurvey_responses = pd.read_excel(\"data/fcc-new-coder-survey.xlsx\")\n\n# View the head of the data frame\nprint(survey_responses.head())\n#>    FreeCodeCamp New Developer Survey Responses, 2016  ...     Unnamed: 97\n#> 0  Source: https://www.kaggle.com/freecodecamp/20...  ...             NaN\n#> 1                                                Age  ...  StudentDebtOwe\n#> 2                                                 28  ...           20000\n#> 3                                                 22  ...             NaN\n#> 4                                                 19  ...             NaN\n#> \n#> [5 rows x 98 columns]# Read spreadsheet and assign it to survey_responses\nsurvey_responses = pd.read_excel(\"data/fcc-new-coder-survey.xlsx\",skiprows = 2)\n\n# View the head of the data frame\nprint(survey_responses.head())\n#>     Age  AttendedBootcamp  ...              SchoolMajor  StudentDebtOwe\n#> 0  28.0               0.0  ...                      NaN           20000\n#> 1  22.0               0.0  ...                      NaN             NaN\n#> 2  19.0               0.0  ...                      NaN             NaN\n#> 3  26.0               0.0  ...  Cinematography And Film            7000\n#> 4  20.0               0.0  ...                      NaN             NaN\n#> \n#> [5 rows x 98 columns]# Create string of lettered columns to load\ncol_string = \"AD, AW:BA\"\n\n# Load data with skiprows and usecols set\nsurvey_responses = pd.read_excel(\"data/fcc-new-coder-survey.xlsx\", \n                        skiprows = 2, \n                        usecols = col_string)\n\n# View the names of the columns selected\nprint(survey_responses.columns)\n#> Index(['ExpectedEarning', 'JobApplyWhen', 'JobPref', 'JobRelocateYesNo',\n#>        'JobRoleInterest', 'JobWherePref'],\n#>       dtype='object')"},{"path":"flat-files.html","id":"multiple-worksheet","chapter":"2 Flat Files","heading":"2.2.2 Multiple worksheet","text":"如果excel檔裡面有很多個sheet，那可以只選一個來讀，或讀多個。語法都是透過sheet_name這個argument\nsheet_name = \"HAHA\" 讀HAHA這個名稱的sheet\nsheet_name = 0 讀index = 0這個sheet，也就是第一個sheet (注意這邊還是zero-indexed系統)\nsheet_name = [\"HAHA\", \"HIHI\"] 用list來說明要讀哪些sheet\nsheet_name = [0, 1] 用list來說明要讀哪些sheet\nsheet_name = None，就可以讀進”全部”的sheets\n如果excel檔裡面有很多個sheet，那可以只選一個來讀，或讀多個。語法都是透過sheet_name這個argumentsheet_name = \"HAHA\" 讀HAHA這個名稱的sheetsheet_name = 0 讀index = 0這個sheet，也就是第一個sheet (注意這邊還是zero-indexed系統)sheet_name = [\"HAHA\", \"HIHI\"] 用list來說明要讀哪些sheetsheet_name = [0, 1] 用list來說明要讀哪些sheetsheet_name = None，就可以讀進”全部”的sheets如果今天讀進多個sheet，那python要用哪種data structure來存？ -> dictionary。key是sheet name，value是dataframe如果今天讀進多個sheet，那python要用哪種data structure來存？ -> dictionary。key是sheet name，value是dataframe現在這兩個sheet，其實欄位都一樣，只是所屬年份不同。那我如果想把這兩個sheet合併起來，然後多加一個column，註明他是從哪個sheet來的，我可以這樣做：簡單畫個圖:","code":"# Load both the 2016 and 2017 sheets by name\nall_survey_data = pd.read_excel(\"data/fcc-new-coder-survey.xlsx\",\n                                skiprows = 2, \n                                sheet_name = ['2016','2017'])\n\n# View the data type of all_survey_data\nprint(type(all_survey_data))\n#> <class 'dict'>print(all_survey_data[\"2017\"])\n#>       Age  AttendedBootcamp  ...                  SchoolMajor  StudentDebtOwe\n#> 0    27.0               0.0  ...                          NaN             NaN\n#> 1    34.0               0.0  ...                          NaN             NaN\n#> 2    21.0               0.0  ...                          NaN             NaN\n#> 3    26.0               0.0  ...                          NaN             NaN\n#> 4    20.0               0.0  ...       Information Technology             NaN\n#> ..    ...               ...  ...                          ...             ...\n#> 995  30.0               0.0  ...                    Chemistry             NaN\n#> 996  31.0               0.0  ...         Computer Programming          1500.0\n#> 997  16.0               0.0  ...                          NaN             NaN\n#> 998  28.0               0.0  ...                      Finance         20000.0\n#> 999  22.0               0.0  ...  Computer Aided Design (CAD)             NaN\n#> \n#> [1000 rows x 98 columns]# Create an empty data frame\ncombined_responses = pd.DataFrame()\n\n# Set up for loop to iterate through values in responses\nfor df in all_survey_data.values():\n  # Print the number of rows being added\n  print(\"Adding {} rows\".format(df.shape[0]))\n  # Append df to all_responses, assign result\n  combined_responses = combined_responses.append(df)\n#> Adding 1000 rows\n#> Adding 1000 rows\nprint(combined_responses)\n#>       Age  AttendedBootcamp  ...                  SchoolMajor  StudentDebtOwe\n#> 0    28.0               0.0  ...                          NaN           20000\n#> 1    22.0               0.0  ...                          NaN             NaN\n#> 2    19.0               0.0  ...                          NaN             NaN\n#> 3    26.0               0.0  ...      Cinematography And Film            7000\n#> 4    20.0               0.0  ...                          NaN             NaN\n#> ..    ...               ...  ...                          ...             ...\n#> 995  30.0               0.0  ...                    Chemistry             NaN\n#> 996  31.0               0.0  ...         Computer Programming          1500.0\n#> 997  16.0               0.0  ...                          NaN             NaN\n#> 998  28.0               0.0  ...                      Finance         20000.0\n#> 999  22.0               0.0  ...  Computer Aided Design (CAD)             NaN\n#> \n#> [2000 rows x 98 columns]# Graph employment statuses in sample\ncounts = combined_responses.groupby(\"EmploymentStatus\").EmploymentStatus.count()\ncounts.plot.barh()\nplt.show()"},{"path":"flat-files.html","id":"modify-imports-truefalse-data","chapter":"2 Flat Files","heading":"2.2.3 Modify imports: True/False data","text":"在pandas中，如果你有些欄位，不是True/False的coding，但他實際上是boolean，那我該如何修改他呢？舉例來說，這筆假資料：可以看到aa是boolean，而bb和cc都不是那如果我希望bb也是boolean，然後”Yes” = True, “” = False，我可以這樣做：nice，修改成功。那如果我想把cc也改成boolean，然後1 = True, 0 = False，我就依樣畫葫蘆：搞定!!","code":"boolean_df = pd.read_excel(\"data/boolean_excel.xlsx\")\nprint(boolean_df)\n#>       aa   bb  cc\n#> 0   True  Yes   1\n#> 1  False   No   1\n#> 2   True   No   0boolean_df.dtypes\n#> aa      bool\n#> bb    object\n#> cc     int64\n#> dtype: objectboolean_df = pd.read_excel(\"data/boolean_excel.xlsx\",\n                          dtype = {\"bb\": \"bool\"},\n                          true_values = [\"Yes\"],\n                          false_values = [\"No\"])\nprint(boolean_df)\n#>       aa     bb  cc\n#> 0   True   True   1\n#> 1  False  False   1\n#> 2   True  False   0\nprint(boolean_df.dtypes)\n#> aa     bool\n#> bb     bool\n#> cc    int64\n#> dtype: objectboolean_df = pd.read_excel(\"data/boolean_excel.xlsx\",\n                          dtype = {\"bb\": \"bool\", \"cc\": \"bool\"},\n                          true_values = [\"Yes\", 1],\n                          false_values = [\"No\", 0])\nprint(boolean_df)\n#>       aa     bb     cc\n#> 0   True   True   True\n#> 1  False  False   True\n#> 2   True  False  False\nprint(boolean_df.dtypes)\n#> aa    bool\n#> bb    bool\n#> cc    bool\n#> dtype: object"},{"path":"flat-files.html","id":"modify-imports-parsing-dates","chapter":"2 Flat Files","heading":"2.2.4 Modify imports: parsing dates","text":"接下來要處理時間的資料了，看一下假資料：可以看到Part1StartTime是標準的yyyy-mm-dd hh:mm:ss，但Part2StartDate只有yyyy-mm-dd，而Part2EndTime更是倒過來的ddmmyyyy hh:mm:ss可以看到Part1StartTime是標準的yyyy-mm-dd hh:mm:ss，但Part2StartDate只有yyyy-mm-dd，而Part2EndTime更是倒過來的ddmmyyyy hh:mm:ss那我們要如何parse呢？ 分兩種做法，一種是讀進來的時候直接parse，另一種是像現在這樣，先讀取，再轉換那我們要如何parse呢？ 分兩種做法，一種是讀進來的時候直接parse，另一種是像現在這樣，先讀取，再轉換我們現在先示範先讀取再轉換的做法。方法很簡單，用pd.to_datetime(column, format = \"\")來處理我們現在先示範先讀取再轉換的做法。方法很簡單，用pd.to_datetime(column, format = \"\")來處理其中，format的部分，對照表如下：\nCode\nMeaning\nExample\n%Y\nYear(4-digit)\n2020\n%m\nMonth (zero-padded)\n10\n%d\nDay (zero-padded)\n08\n%H\nHour (24-hour clock)\n13\n%M\nMinute (zero-padded)\n37\n%S\nSecond(zero-padded)\n12\n其中，format的部分，對照表如下：接下來第二種作法，是讀取時，直接指定：","code":"fake_time_dic = {\n  \"Part1StartTime\": [\"2020-10-07 21:00:00\", \"2020-10-08 21:00:00\"],\n  \"Part1EndTime\": [\"2020-10-07 22:00:00\", \"2020-10-08 22:00:00\"],\n  \"Part2StartDate\": [\"2020-11-07\", \"2020-11-08\"],\n  \"Part2StartTime\": [\"21:00:00\", \"22:12:15\"],\n  \"Part2EndTime\": [\"07112020 22:00:00\", \"08112020 23:00:00\"]\n}\n\nfake_time_df = pd.DataFrame(fake_time_dic)\n# fake_time_df.to_excel(\"/home/tom-hanks/import_data/fake_time_df.xlsx\")\npy$fake_time_df\n#>        Part1StartTime        Part1EndTime Part2StartDate\n#> 1 2020-10-07 21:00:00 2020-10-07 22:00:00     2020-11-07\n#> 2 2020-10-08 21:00:00 2020-10-08 22:00:00     2020-11-08\n#>   Part2StartTime      Part2EndTime\n#> 1       21:00:00 07112020 22:00:00\n#> 2       22:12:15 08112020 23:00:00fake_time_df['Part1StartTime'] = pd.to_datetime(fake_time_df['Part1StartTime'], format = \"%Y-%m-%d %H:%M:%S\")\nfake_time_df['Part2EndTime'] = pd.to_datetime(fake_time_df['Part2EndTime'], format = \"%m%d%Y %H:%M:%S\")\n\nfake_time_df.dtypes\n#> Part1StartTime    datetime64[ns]\n#> Part1EndTime              object\n#> Part2StartDate            object\n#> Part2StartTime            object\n#> Part2EndTime      datetime64[ns]\n#> dtype: objectfake_time_df = pd.read_excel(\"data/fake_time_df.xlsx\", \n                            parse_dates = [\"Part1StartTime\"])\n# Print first few values of Part1StartTime\nprint(fake_time_df.Part1StartTime.head())\n#> 0   2020-10-07 21:00:00\n#> 1   2020-10-08 21:00:00\n#> Name: Part1StartTime, dtype: datetime64[ns]# Create dict of columns to combine into new datetime column\ndatetime_cols = {\"Part2Start\": [\"Part2StartDate\", \n                                \"Part2StartTime\"]}\n\n# Load file, supplying the dict to parse_dates\nfake_time_df = pd.read_excel(\"data/fake_time_df.xlsx\",\n                            parse_dates=datetime_cols)\n\n# View summary statistics about Part2Start\nprint(fake_time_df.Part2Start.describe())\n#> count                       2\n#> unique                      2\n#> top       2020-11-07 21:00:00\n#> freq                        1\n#> first     2020-11-07 21:00:00\n#> last      2020-11-08 22:12:15\n#> Name: Part2Start, dtype: object\n#> \n#> <string>:1: FutureWarning: Treating datetime data as categorical rather than numeric in `.describe` is deprecated and will be removed in a future version of pandas. Specify `datetime_is_numeric=True` to silence this warning and adopt the future behavior now."},{"path":"database.html","id":"database","chapter":"3 Database","heading":"3 Database","text":"這一節只講如何連SQLite，他是把DB的檔存在local computer上的作法，所以使用上，等於是直接去開啟此DB的檔，然後開啟後裡面有多個表格，就可以開始單表格查詢或多表格合併了使用的package是sqlalchemy，就先把他想成R的RJDBC，此套件可以連各種DB，只是這堂課是介紹連SQLite而已但實際上，工作上我需要的教我連sql server或oracle，而不是這種SQLite，所以這一節我就放個連SQLite的範例，其他我就不想整理了(這一節其他內容就是一直教SQL語法而已)，等另個課程(專門講連db的)，再好好整理連各DB的方式","code":""},{"path":"database.html","id":"建立連線用sqlalchemy.create_engine","chapter":"3 Database","heading":"3.1 建立連線用sqlalchemy.create_engine()","text":"","code":"# Import sqlalchemy's create_engine() function\nfrom sqlalchemy import create_engine\nimport pandas as pd# Create the database engine\nengine = create_engine('sqlite:///data/data.db')\n\n# View the tables in the database\nprint(engine.table_names())\n#> ['boro_census', 'hpd311calls', 'weather']\n#> \n#> <string>:1: SADeprecationWarning: The Engine.table_names() method is deprecated and will be removed in a future release.  Please refer to Inspector.get_table_names(). (deprecated since: 1.4)"},{"path":"database.html","id":"撈資料用pd.read_sqlquery_text-engine","chapter":"3 Database","heading":"3.2 撈資料用pd.read_sql(query_text, engine)","text":"如果我想撈weather資料表的內容，我可以這樣做：","code":"# Create a SQL query to load the entire weather table\nquery = \"\"\"\nselect * from weather;\n\"\"\"\n\n# Load weather with the SQL query\nweather = pd.read_sql(query, engine)\n\n# View the first few rows of data\nprint(weather.head())\n#>        station                         name  latitude  ...  tavg  tmax tmin\n#> 0  USW00094728  NY CITY CENTRAL PARK, NY US  40.77898  ...          52   42\n#> 1  USW00094728  NY CITY CENTRAL PARK, NY US  40.77898  ...          48   39\n#> 2  USW00094728  NY CITY CENTRAL PARK, NY US  40.77898  ...          48   42\n#> 3  USW00094728  NY CITY CENTRAL PARK, NY US  40.77898  ...          51   40\n#> 4  USW00094728  NY CITY CENTRAL PARK, NY US  40.77898  ...          61   50\n#> \n#> [5 rows x 13 columns]"},{"path":"json-apis.html","id":"json-apis","chapter":"4 JSON & APIs","heading":"4 JSON & APIs","text":"","code":""},{"path":"json-apis.html","id":"load-json-data","chapter":"4 JSON & APIs","heading":"4.1 Load JSON data","text":"這邊先介紹JSON的三種組成方式：","code":""},{"path":"json-apis.html","id":"record-orientation","chapter":"4 JSON & APIs","heading":"4.1.1 Record Orientation","text":"record就是observation的意思，就是row的意思所以把dataframe的一個一個row，組成的json，就叫row orientation/ record orientation這是最常看到的json格式，也可以說他是list dictionaries","code":"[\n  {\n    \"name\": \"Hank\",\n    \"age\": 33,\n    \"height\": 185\n  },\n  {\n    \"name\": \"Pinpin\"\n    \"age\": 33,\n    \"height\": 160\n  },\n  {\n    \"name\": \"Zoe\",\n    \"age\": 37,\n    \"height\": 166\n  }\n]"},{"path":"json-apis.html","id":"column-orientation","chapter":"4 JSON & APIs","heading":"4.1.2 Column Orientation","text":"這就是用dataframe的一個一個column，所組成的json，所以叫column orientation你也可以稱它為dictionary lists","code":"{\n  \"name\": [\"Hank\", \"Pinpin\", \"Zoe\"],\n  \"age\": [33, 33, 37],\n  \"height\": [185, 160, 166]\n}"},{"path":"json-apis.html","id":"split-orientation","chapter":"4 JSON & APIs","heading":"4.1.3 Split Orientation","text":"這就是把dataframe的column_name(簡稱column), row_name(簡稱index), value(簡稱data)這三塊分開放的格式：如果你已經有.json檔了，可以這樣讀檔：\n對index orientation 或 column orientation來說： pd.read_json(\"data.json\")\n對split orientation來說： pd.read_json(\"data.json\", orient = \"split\")\n對index orientation 或 column orientation來說： pd.read_json(\"data.json\")對split orientation來說： pd.read_json(\"data.json\", orient = \"split\")","code":"{\n  \"column\": [\"name\", \"age\", \"height\"],\n  \"index\": [0, 1, 2],\n  \"data\": [\n    [\"Hank\", 33, 185],\n    [\"Pinpin\", 33, 160],\n    [\"Zoe\", 37, 166]\n  ]\n}import pandas as pd# Load the daily report to a data frame\npop_in_shelters = pd.read_json(\"dhs_daily_report.json\")\n\n# View summary stats about pop_in_shelters\nprint(pop_in_shelters.describe())"},{"path":"json-apis.html","id":"introduction-to-apis","chapter":"4 JSON & APIs","heading":"4.2 Introduction to APIs","text":"剛剛我們是直接開啟local端的.json檔，那現在要用API，去開啟server那邊的.json檔，就差在這而已所以這邊要學的，就是如何透過server端的API，去get遠端的資料，而遠端的資料，通常都是用JSON在儲存的，所以就是讀遠端的.json檔的意思而已我們將使用request這個套件來處理，以get method舉例，會寫成： response = requests.get(url_string, params, headers,...)其中，各parameter的意思如下：\nurl_string: 就填入該API的url，為字串格式，例如：“https://172.16.99.233:8000/predict”\nparams: 為dictionary格式，填入客製化的參數(如果原本要get的那個url所用的function，不需要你填參數，那這個argument就不用下)。例如：剛剛的url_string如果是讓你去get一筆data，然後參數要你下location=“xxx”，好幫你filter出data中location == “xxx”的資料給你，那這時的params就要寫成params = {\"location\": \"xxx\"}\nheaders: 填入authentication資訊，為dictionary格式\nurl_string: 就填入該API的url，為字串格式，例如：“https://172.16.99.233:8000/predict”params: 為dictionary格式，填入客製化的參數(如果原本要get的那個url所用的function，不需要你填參數，那這個argument就不用下)。例如：剛剛的url_string如果是讓你去get一筆data，然後參數要你下location=“xxx”，好幫你filter出data中location == “xxx”的資料給你，那這時的params就要寫成params = {\"location\": \"xxx\"}headers: 填入authentication資訊，為dictionary格式output的object: respone，包含data和metadata兩部分\nresponse.json()會把get下來的JSON資料，轉成dictionary給你。所以你可以用pd.DataFrame()來parse他。記得，不要用pd.read_json()，因為後者吃的是字串，但你現在的資料是dictionary了\nresponse.json()會把get下來的JSON資料，轉成dictionary給你。所以你可以用pd.DataFrame()來parse他。記得，不要用pd.read_json()，因為後者吃的是字串，但你現在的資料是dictionary了現在來練習吧，課程中使用”Yelp Business Search API cafes New York City”這個API，完整流程示範如以下的code，但因為認證的key已經過期，現在無法操作了：那沒關係，反正只是要練習從遠端拉.json資料，那我可以改用爬蟲的方式來練習我從pchome的網站中，搜尋商品後，發現他show給你的網頁，背後是讀這個路徑的.json資料：“http://ecshweb.pchome.com.tw/search/v3.3//results?q=sony&page=1&sort=rnk/dc”那太棒拉，我就get他就好，而且此時不用下parameter了，因為他已經結合在url裡面，也不用下header，因為不需要認證看到這個JSON包含超多資訊，簡要一點看，他包含哪幾大塊：了解，那我要的，其實只是”prods”的部分，看一下這一塊的資料很明顯的是一個record orientation(或說，list dictionaries)的格式，那我直接用pd.DataFrame()就可以把他parse成dataframe了搞定拉！","code":"import requests\n\napi_url = \"https://api.yelp.com/v3/businesses/search\"\nparams = {'location': 'NYC', 'term': 'cafe'}\nheaders = {\n  'Authorization': 'Bearer mhmt6jn3SFPVC1u6pfwgHWQvsa1wmWvCpKRtFGRYlo4mzA14SisQiDjyygsGMV2Dm7tEsuwdC4TYSA0Ai_GQTjKf9d5s5XLSNfQqdg1oy7jcBBh1i7iQUZBujdA_XHYx'\n}\n\n# Get data about NYC cafes from the Yelp API\nresponse = requests.get(api_url, \n                headers=headers, \n                params=params)\n\n# Extract JSON data from the response\ndata = response.json()\n\n# Load data to a data frame\ncafes = pd.DataFrame(data[\"businesses\"])\n\n# View the data's dtypes\nprint(cafes.dtypes)import requests \nurl = \"http://ecshweb.pchome.com.tw/search/v3.3/all/results?q=sony&page=1&sort=rnk/dc\"\nresponse = requests.get(url)\ndata = response.json()\ndata\n#> {'QTime': 37, 'totalRows': 32597, 'totalPage': 100, 'range': {'min': '', 'max': ''}, 'cateName': '', 'q': 'sony', 'subq': '', 'token': ['sony'], 'isMust': 1, 'prods': [{'Id': 'DEBB55-A900AXCHG', 'cateId': 'DEBB55', 'picS': '/items/DEBB55A900AXCHG/000002_1642059796.jpg', 'picB': '/items/DEBB55A900AXCHG/000001_1642059795.jpg', 'name': 'PX大通HR7PRO星光夜視行車紀錄器SONY感光元件GPS區間測速記錄器', 'describe': '每日強檔▼瘋殺特賣PX大通HR7PRO星光夜視旗艦王汽車行車紀錄器真HDR高動態SONY STARVIS感光元件GPS區間測速記錄器贈32G記憶卡\\\\r\\\\n《只有一天★限時狂降》\\r\\n開始﹕０１／１３(星期四)１０：００\\r\\n結束﹕０１／１４(星期五)１０：００\\r\\n網路價$4988．限時價↘$４１８８\\r\\n滿＄３０００再送３００ｐ幣\\r\\n======================================\\r\\n\\r\\n🚩【夜視新標竿】: hdr高動態錄影 +sony starvis星光級感光元件。\\r\\n>>真hdr:抗led或hid高強光、車牌影像清晰不過曝 starvis:提升整體亮度、解決環境過暗                \\r\\n🚩【安全新標竿】: ts碼流存檔機制。\\r\\n>>以秒截檔設計，確保錄影檔案不毀損  \\r\\n🚩【好用新標竿】: 一鍵進格式化。\\r\\n>>動動手指，輕鬆格式化    \\r\\n🚩【測速新標竿】: gps區間+定點測速雙警示。\\r\\n>>gps雙衛星，雙預警加倍安心\\r\\n\\r\\n👍👍👍多位論壇部落客一致推薦👍👍👍\\r\\n★pchome開箱focus專欄★大力推薦\\r\\n★tony的玩樂生活筆記★部落客開箱推薦\\r\\n★小老婆汽機車資訊網★網路車類媒體大力推薦\\r\\n★正宅爸3c開箱★部落客開箱推薦\\r\\n★星星陳世章★部落客開箱推薦\\r\\n★寶井秀人hyde★部落客開箱推薦\\r\\n★basic施鈞程★部落客開箱推薦\\r\\n★硬是要學★部落客開箱推薦\\r\\n★鑫部落★部落客開箱推薦\\r\\n★另有其他規格款式可選↓↓↓\\r\\n\\r\\n《★鑽石版頂規 ★ 》\\r\\n《★hr7 pro》\\r\\n《★鈦金版高規 ★ 》\\r\\n《★hr7g》\\r\\n《★白金版均規 ★ 》\\r\\n《★hr7》\\r\\n\\r\\n※六大優勢\\r\\n1. hdr高動態錄影+sony starvis星光夜視技術>>抗led強光、夜間車牌清晰\\r\\n2. ts碼流存檔機制>>以秒截檔設計，不正常斷電、車禍撞擊，錄影檔案不毀損\\r\\n3. gps區間測速>>警示功能提升\\r\\n4. 2吋ips高清螢幕>>廣視角，資訊顯示清楚\\r\\n5. 不掉落魔法貼>>全新支架設計，機身更穩固\\r\\n6. 一鍵進格式化>>簡易操作，確保錄影成功率\\r\\n\\r\\n※完整特色\\r\\n▼ 1080p 30fps hdr高動態\\r\\n▼ sony starvis超級星光夜視\\r\\n▼ f 1.4超大光圈+7玻1ir鏡頭\\r\\n▼ 1080p 60fps wdr寬動態\\r\\n▼ ips廣視角高清螢幕\\r\\n▼ 以秒截檔ts碼流存檔機制\\r\\n▼ 行車魔法貼不掉落支架\\r\\n▼ 148度大廣角\\r\\n▼ hud抬頭顯示\\r\\n▼ 13段曝光值調整\\r\\n▼ gps雙衛星接收快速定位\\r\\n▼ gps區間+定點測速照相雙預警\\r\\n▼ g-sensor碰撞自動偵測鎖檔\\r\\n▼ 一鍵鎖檔備份關鍵檔案 \\r\\n▼ 一鍵快捷啟動記憶卡格式化\\r\\n▼ 一鍵快速拍照\\r\\n▼ 記憶卡異常提醒\\r\\n▼ 記憶卡30天定期格式化提醒', 'price': 4188, 'originPrice': 4188, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DGBJGB-A900CAR5E', 'cateId': 'DGBJGB', 'picS': '/items/DGBJGBA900CAR5E/000002_1641459421.jpg', 'picB': '/items/DGBJGBA900CAR5E/000001_1641459421.jpg', 'name': 'SONY PS5原廠 DualSense 無線控制器-銀河紫 CFI-ZCT1G04', 'describe': 'SONY PS5原廠 DualSense 無線控制器銀河紫 CFI-ZCT1G04\\\\r\\\\n來自銀河系的新色 為遊戲之夜點燃熱情 \\r\\n創新ps5控制器 深入栩栩如生遊戲體驗 \\r\\n■搭載觸覺回饋和自適應扳機，更身歷其境的感受\\r\\n■直覺配置搭配強化操作桿，讓你掌握全局\\r\\n■觸覺回饋真實感受到武器後座力等動態震動\\r\\n■自適應扳機體驗往後拉弓、急剎車等力量與張力\\r\\n■內建麥克風與3.5mm耳機連接端\\r\\n■帶來更多方式向全世界播送冒險經歷\\r\\n\\r\\n☛看看其他銀河系新色-星塵紅', 'price': 2280, 'originPrice': 2280, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': ['C025307', 'C025308'], 'BU': 'ec'}, {'Id': 'DMAAEC-A900BFLRR', 'cateId': 'DMAAEC', 'picS': '/items/DMAAECA900BFLRR/000002_1622769045.jpg', 'picB': '/items/DMAAECA900BFLRR/000001_1622769045.jpg', 'name': 'SONY 索尼  5.1 聲道 SOUNDBAR 家庭劇院組 HT-S40R', 'describe': 'SONY 索尼 5.1 聲道 SOUNDBAR 家庭劇院組 HT-S40R\\\\r\\\\n完整的 5.1 聲道，並具備 600 w 震撼輸出\\r\\n支援 dolby® digital 音訊解碼\\r\\n無線後環繞喇叭\\r\\nbluetooth® 連線功能支援音樂串流\\r\\n支援 hdmi arc、光纖與 3.5mm 輸入，即可輕鬆設定', 'price': 9900, 'originPrice': 9900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DPAD06-A900BFGWY', 'cateId': 'DPADZ9', 'picS': '/items/DPAD06A900BFGWY/000002_1630490536.jpg', 'picB': '/items/DPAD06A900BFGWY/000001_1630490536.jpg', 'name': '[21年新機上市] Sony BRAVIA 65吋 4K Google TV 顯示器 XRM-65X90J', 'describe': '送精緻安裝[21年新機上市] SONY  BRAVIA 65吋 4K Google TV 顯示器 XRM-65X90J\\\\r\\\\n★原廠註冊送統一超商2000元虛擬商品卡★65x90j_4k google tv\\r\\n\\r\\n1.全陣列 led 背光\\r\\n2.認知智慧處理器xr\\r\\n3.xr原色顯示 pro\\r\\n4.xr對比增強5\\r\\n5.google tv可自由下載應用程式(如disney+等)\\r\\n6.多音域環繞聲場技術\\r\\n7.提供多種尺寸選擇 : 75”  65”  55\"  50\"\\r\\n\\r\\n★保固：二年\\r\\n\\r\\n\\r\\n選購50吋賣場★sony bravia 50吋 4k顯示器 xrm-50x90j\\r\\n選購55吋賣場★sony bravia 55吋 4k顯示器 xrm-55x90j\\r\\n選購75吋賣場★sony bravia 75吋 4k顯示器 xrm-75x90j\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★★\\r\\n【退貨&配送注意事項】\\r\\n●收到商品時，請先檢查外包裝是否完整。\\r\\n●鑑賞期内請勿註冊商品或開通贈品，否則不予退貨。\\r\\n●產品到貨 7 天猶豫期之權益，但猶豫期並非試用期，退回產品必須是回復原狀，亦即必須回復至您收到商品時:\\r\\n1.原始狀態（無刮傷、無髒、無字跡）。\\r\\n2.包裝完整(保持產品附件、包裝、廠商紙箱及所有附随文件或資料之完整性)否則退貨刮傷損壞，需負擔整新費用以售價30%計算，將由訂購人支付給出貨廠商，如商品嚴重受損以實際報價為準。', 'price': 56900, 'originPrice': 56900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DYAQBC-A900BNLZQ', 'cateId': 'DYAQF4', 'picS': '/items/DYAQBCA900BNLZQ/000002_1641867255.jpg', 'picB': '/items/DYAQBCA900BNLZQ/000001_1641867254.jpg', 'name': '【SONY 索尼】WF-1000XM4 主動式降噪真無線藍牙耳機 智慧降噪 / IPX4防水(公司貨保固18+6個月)', 'describe': '▲限時加贈7-11超商商品卡300元▲【SONY 索尼】WF-1000XM4 主動式降噪真無線藍牙耳機 智慧降噪 / IPX4防水(公司貨保固12+6個月) \\\\r\\\\n《絕不手軟29折起》 \\r\\n開始：１／０７（五）１１：００\\r\\n結束：１／２０（四）１０：５９\\r\\n網路價$７９９０．\\r\\n限時價↘$７４９０\\r\\nsony 首款符合 hires wireless 標準的真無線耳機\\r\\n支援 ldac 傳輸協定，提供高品質音樂\\r\\n全新整合處理器 v1，提升主動降噪效能\\r\\n全新噪音隔離耳塞，提升被動降噪效果\\r\\n更清晰的通話品質\\r\\n自然的環境聲表現\\r\\nipx4 防水等級', 'price': 7490, 'originPrice': 7490, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DMAAEC-A900C2KD1', 'cateId': 'DMAAEC', 'picS': '/items/DMAAECA900C2KD1/000002_1638953189.jpg', 'picB': '/items/DMAAECA900C2KD1/000001_1638953189.jpg', 'name': 'SONY 索尼 HT-A7000  DOLBY ATMOS  7.1.2聲道家庭劇院+SA-RS3S+SA-SW3', 'describe': 'SONY 索尼  HT-A7000 DOLBY ATMOS 7.1.2聲道 家庭劇院+SA-RS3S+SA-SW3\\\\r\\\\n商品含:sa-rs3s 無線後環繞+sa-sw3無線重低音', 'price': 54900, 'originPrice': 54900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DYAQ0R-A900BG7DX', 'cateId': 'DYAQ0X', 'picS': '/items/DYAQ0RA900BG7DX/000002_1641868504.jpg', 'picB': '/items/DYAQ0RA900BG7DX/000001_1641868504.jpg', 'name': 'SONY WF-1000XM4 黑色 降噪真無線耳機', 'describe': '▼尾牙大牌檔★夢幻清單▼★註冊送KKBOX60天再送5%P幣★SONY WF-1000XM4 黑色 降噪真無線耳機《公司貨註冊保固1年+6個月》\\\\r\\\\n《禮品選購大公開！全場下殺》\\r\\n開始：０１／１１（二）１１：００\\r\\n結束：０１／１８（二）１１：００\\r\\n網路價$７９９０．\\r\\n限時價↘$７４９０送５％ｐ幣\\r\\n☆註冊送kkbox無損60天儲值卡☆\\r\\n★即日起至2022 2 13註冊送★\\r\\n1. kkbox無損60天儲值卡 (此為原廠活動贈品，僅送乙組 顆。請至sony官網登記，贈品將於補寄，造成不便敬請見諒。) \\r\\n\\r\\n\\r\\n■ 台灣sony公司貨 官網註冊登入保固 (依官網公布為準)\\r\\n■ 先進的數位降噪與整合式處理器 v1\\r\\n■ 無線高解析音質的音質\\r\\n■ 智慧聆聽和清晰的通話品質\\r\\n■ 人體工學表面設計可穩固貼合\\r\\n■ 防水設計和長效電池續航力\\r\\n■ ncc許可字號:ccao21lp0490t1\\r\\n*耳塞保養祕訣：請使用乾布擦拭髒污，請勿使用酒精或濕紙巾，會加速耳塞的耗損程度。\\r\\n\\r\\n★銀色賣場請點我\\r\\n\\r\\n\\r\\n★sony原廠商品皆為封膜式包裝，確定商品購買前請勿拆封，若拆封後退貨，商品無法復原整新 將有可能費用產生，造成您的不便深感抱歉，謝謝!', 'price': 7490, 'originPrice': 7490, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DMAAEC-A900C2EF7', 'cateId': 'DMAAEC', 'picS': '/items/DMAAECA900C2EF7/000002_1638858885.jpg', 'picB': '/items/DMAAECA900C2EF7/000001_1638858885.jpg', 'name': 'SONY 索尼 HT-A7000  DOLBY ATMOS  7.1.2聲道家庭劇院', 'describe': 'SONY 索尼  HT-A7000 DOLBY ATMOS 7.1.2聲道 家庭劇院\\\\r\\\\n7.1.2 聲道支援 dolby atmos dts:x', 'price': 39900, 'originPrice': 39900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DCAY7T-A900AWDDU', 'cateId': 'DCAY7T', 'picS': '/items/DCAY7TA900AWDDU/000002_1602145312.jpg', 'picB': '/items/DCAY7TA900AWDDU/000001_1642122889.jpg', 'name': 'SONY 耳罩式耳機 WH-1000XM4 無線藍牙 HD降噪 音質升級 降噪優化【保固一年】', 'describe': '▼每日強檔‧瘋殺特賣▼SONY 耳罩式耳機 WH-1000XM4 無線藍牙/有線兩用 HD降噪 音質升級 降噪優化【保固一年】\\\\r\\\\n《限時狂降★週一10點回價》\\r\\n開始﹕０１／１４(星期五)１０：００  \\r\\n結束﹕０１／１７(星期一)１０：００\\r\\n原價$10900．限時價↘$8590限時限量 把握機會不要錯過!\\r\\n\\r\\n\\r\\n【01 14~01 16 每日10點開搶 名額有限】\\r\\n全周邊滿$2500最高送15%p幣\\r\\n●玉山pi卡綁定pi錢包保證5%，無上限.\\r\\n●不限支付保證10%，最高送250p幣(限量~送完為止).\\r\\n\\r\\n● 附原廠耳機線 可有線 無線兩用\\r\\n● 智能免摘 speak to chat 聊天模式\\r\\n● 可同時與兩組裝置連線\\r\\n● 全新dsee extreme音質還原技術與ldac提供高品質音訊\\r\\n● hd 降噪處理器qn1領先的降噪技術\\r\\n★更多品牌精選耳機★\\r\\n★harman kardon 防水頸掛式耳機 fly bt →點我 go\\r\\n★sony 耳罩式降噪耳機 wh-1000xm4 公司貨藍色 →點我 go', 'price': 8590, 'originPrice': 8590, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DYAW4G-A900BSZ9H', 'cateId': 'DYAW4G', 'picS': '/items/DYAW4GA900BSZ9H/000002_1641139994.jpg', 'picB': '/items/DYAW4GA900BSZ9H/000001_1642125793.jpg', 'name': 'SONY Xperia 5 III (8G/256G)', 'describe': '贈SONY運動入耳式耳機WI-SP510N+原廠背包+殼貼+好禮二選一SONY Xperia 5 III #玩美合手旗艦\\\\r\\\\n6.1 吋 21：9 比例 hdr oled 螢幕 120hz 更新率、240hz 觸控採樣率\\r\\nram 8gb rom 256gb\\r\\n5g + 4g 雙卡雙待\\r\\nqualcomm® 驍龍™ 888 \\r\\n深色背景強化模式\\r\\n具防水功能 (ip65 68)\\r\\n高達 105mm 的光學望遠鏡頭\\r\\n4500mah 電池\\r\\n正反面gorilla glass 6 玻璃\\r\\n360 實景音效音樂 360 空間模擬音效\\r\\n適用於人類與動物的即時眼部偵測自動對焦功能', 'price': 24990, 'originPrice': 24990, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DPAD06-A900BL9KG', 'cateId': 'DPADZ9', 'picS': '/items/DPAD06A900BL9KG/000002_1636019387.jpg', 'picB': '/items/DPAD06A900BL9KG/000001_1636019387.jpg', 'name': '【SONY 索尼】BRAVIA 65型 4K OLED Google TV 顯示器 XRM-65A90J', 'describe': '[21年新上市] Sony BRAVIA  65型 4K OLED Google TV 顯示器 XRM-65A90J 《送基本安裝》\\\\r\\\\n1. 獨一無二的 oled 面板技術，呈現純粹黑色與驚人的對比度。\\r\\n2. 認知智慧處理器xr\\r\\n3. xr oled 對比增強\\r\\n4. 平面聲場技術進階版\\r\\n5. google tv: 全新使用者介面，容易操作，迎接無限串流娛樂體驗。\\r\\n6. 提供多種尺寸選擇: 65”、55“', 'price': 154900, 'originPrice': 154900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DGBJJ3-A900BSV89', 'cateId': 'DGBJJ3', 'picS': '/items/DGBJJ3A900BSV89/000002_1632318886.jpg', 'picB': '/items/DGBJJ3A900BSV89/000001_1632318885.jpg', 'name': '【 SONY 索尼 】副廠 PS5 多功能雙層主機全收納包', 'describe': '【  SONY 索尼  】PS5  副廠多功能雙層主機全收納包\\\\r\\\\n雙層款 全收納配件包 \\r\\n●手提 側背皆可使用\\r\\n●穩定主機 收納方便\\r\\n●美觀造型\\r\\n●側邊鍊條加強\\r\\n●尺寸 43cm*22cm*21cn', 'price': 890, 'originPrice': 890, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': ['C024813', 'C025307', 'C025308'], 'BU': 'ec'}, {'Id': 'DGBJJ3-A900B6O9O', 'cateId': 'DGBJJ3', 'picS': '/items/DGBJJ3A900B6O9O/000002_1615435526.jpg', 'picB': '/items/DGBJJ3A900B6O9O/000001_1615435526.jpg', 'name': 'SONY 索尼 PS5 副廠 主機散熱底座 多功能遊戲手把充電支架', 'describe': 'SONY 索尼 PS5 副廠 主機散熱底座 多功能遊戲手把充電支架\\\\r\\\\n充電收納二合一\\r\\n光碟版數位版主機通用\\r\\n金屬底座風扇快速冷卻主機', 'price': 899, 'originPrice': 899, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': ['C024813', 'C025307', 'C025308'], 'BU': 'ec'}, {'Id': 'DMAAF7-A900AEN2S', 'cateId': 'DMAAE0', 'picS': '/items/DMAAF7A900AEN2S/000002_1575876436.jpg', 'picB': '/items/DMAAF7A900AEN2S/000001_1575876436.jpg', 'name': 'SONY 索尼 4K藍光播放機 UBP-X700', 'describe': 'SONY 索尼 4K藍光播放機 UBP-X700\\\\r\\\\n4k藍光播放器，影片場景更逼真\\r\\n廣泛支援視訊 音樂串流服務\\r\\n4k升頻高達60p呈現精采畫面\\r\\n兩個 hdmi 輸出可別運用於\\r\\n視訊與音訊，讓聲音更清晰', 'price': 9900, 'originPrice': 9900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DYAW3U-A900BPYL0', 'cateId': 'DYAW3U', 'picS': '/items/DYAW3UA900BPYL0/000002_1641977420.jpg', 'picB': '/items/DYAW3UA900BPYL0/000001_1641977420.jpg', 'name': 'SONY Xperia 1 III (12G/256G)', 'describe': '▲虎氣滿滿新年來▲★贈+原廠背包+殼貼等好禮SONY Xperia 1 III (12G/256G)\\\\r\\\\n《好運到家18折起》 \\r\\n開始：１／１３（四）１１：００\\r\\n結束：１／２０（四）１０：５９\\r\\n網路價$３３９９０．限時價↘$２９９９０\\r\\n\\r\\n\\r\\n6.5 吋 21:9 cinemawide™ 4k hdr oled 120hz 更新率顯示幕\\r\\nram 12gb rom 256gb\\r\\n5g + 4g 雙卡雙待 \\r\\nqualcomm® 驍龍™ 888 \\r\\n全段立體聲雙揚聲器\\r\\n具防水功能 (ip65 68)\\r\\n高達 105mm 的光學望遠鏡頭\\r\\n4500mah 電池 最高 30w 有線快充\\r\\n提供無線充電、無線電量\\r\\n霧面玻璃背蓋  gorilla glass 6 玻璃\\r\\n240hz 降低動態影像模糊、h.s. 電源控制\\r\\n適用於人類與動物的即時眼部偵測自動對焦功能', 'price': 29990, 'originPrice': 29990, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DYAQ0R-A900BG6WX', 'cateId': 'DYAQ0X', 'picS': '/items/DYAQ0RA900BG6WX/000002_1642151131.jpg', 'picB': '/items/DYAQ0RA900BG6WX/000001_1642151131.jpg', 'name': 'SONY WF-1000XM4 黑色 降噪真無線耳機', 'describe': '★註冊送KKBOX 60天再送5%P幣★SONY WF-1000XM4 黑色 降噪真無線耳機《公司貨註冊保固1年6個月》\\\\r\\\\n送5%p幣活動說明 \\r\\n付款方式：任一付款方式並全額支付。\\r\\n(各商品可用方式以結帳頁標示為主，若以pchome儲值 禮券搭配其他付款方式，不符贈點資格)\\r\\n☆註冊送kkbox無損60天儲值卡☆\\r\\n★即日起至2022 2 13註冊送★\\r\\n1. kkbox無損60天儲值卡 (此為原廠活動贈品，僅送乙組 顆。請至sony官網登記，贈品將於補寄，造成不便敬請見諒。) \\r\\n\\r\\n\\r\\n■ 台灣sony公司貨 官網註冊登入保固 (依官網公布為準)\\r\\n■ 先進的數位降噪與整合式處理器 v1\\r\\n■ 無線高解析音質的音質\\r\\n■ 智慧聆聽和清晰的通話品質\\r\\n■ 人體工學表面設計可穩固貼合\\r\\n■ 防水設計和長效電池續航力\\r\\n■ ncc許可字號:ccao21lp0490t1\\r\\n*耳塞保養祕訣：請使用乾布擦拭髒污，請勿使用酒精或濕紙巾，會加速耳塞的耗損程度。\\r\\n\\r\\n★銀色賣場請點我\\r\\n\\r\\n\\r\\n★sony原廠商品皆為封膜式包裝，確定商品購買前請勿拆封，若拆封後退貨，商品無法復原整新 將有可能費用產生，造成您的不便深感抱歉，謝謝!\\r\\n\\r\\n《24h出貨請至此賣場下單》', 'price': 7490, 'originPrice': 7490, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DMAF03-A900BQDVK', 'cateId': 'DMAF03', 'picS': '/items/DMAF03A900BQDVK/000002_1630636674.jpg', 'picB': '/items/DMAF03A900BQDVK/000001_1630636674.jpg', 'name': 'SONY 索尼 SRS-XB13  EXTRA BASS™ 可攜式無線 藍芽喇叭', 'describe': 'SONY 索尼 SRS-XB13 EXTRA BASS™ 可攜式無線 藍芽喇叭\\\\r\\\\nextra bass 給您深沉強力的音效\\r\\n藍牙連線和 fast pair 技術\\r\\n防水防塵 (ip67 等級)\\r\\n輕巧體積，電池續航力 16 小時', 'price': 1890, 'originPrice': 1890, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DYAQ9D-A900AT4UE', 'cateId': 'DYAQ0R', 'picS': '/items/DYAQ9DA900AT4UE/000002_1641960578.jpg', 'picB': '/items/DYAQ9DA900AT4UE/000001_1641960578.jpg', 'name': 'SONY WH-1000XM4 輕巧無線藍牙降噪耳罩式耳機 - 黑色', 'describe': '送200禮券+購物袋+耳機架SONY WH-1000XM4 黑色 藍牙降噪耳罩式耳機《公司貨註冊保固2年》\\\\r\\\\n■ 台灣sony公司貨保固12個月，官網註冊延長12個月■ 電池續航力 30 個小時■ 獨家研發的 hd 降噪處理器 qn1■ 情境分為旅行、步行和等待 3 種狀態■ 快充 10 分鐘，支撐 5 小時播放\\r\\n▌精選贈品 ▌環保購物袋+7-11禮卷$200+專用原木耳機架乙個(贈品將隨機寄出，若沒收到贈品，請來信告知，將補寄給您，造成不便，請見諒。)\\r\\n貼心小提醒：網購享有7天猶豫期，但非指7天內可\"隨意試用\"\\r\\n★原廠商品包裝，確定商品購買前請勿拆封，若拆封後退貨(瑕疵品除外)，商品無法復原整新，將有可能造成3-5成費用產生，造成您的不便深感抱歉，謝謝!', 'price': 8990, 'originPrice': 8990, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DMAAEC-A900C2KEE', 'cateId': 'DMAAEC', 'picS': '/items/DMAAECA900C2KEE/000002_1638953497.jpg', 'picB': '/items/DMAAECA900C2KEE/000001_1638953497.jpg', 'name': 'SONY 索尼 HT-A7000  DOLBY ATMOS  7.1.2聲道家庭劇院+SA-RS3S+SA-SW5', 'describe': 'SONY 索尼  HT-A7000 DOLBY ATMOS 7.1.2聲道 家庭劇院+SA-RS3S+SA-SW5\\\\r\\\\n商品含:sa-rs3s 無線後環繞+sa-sw5無線重低音', 'price': 67900, 'originPrice': 67900, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DGBJA7-A900B6502', 'cateId': 'DGBJA7', 'picS': '/items/DGBJA7A900B6502/000002_1614764722.jpg', 'picB': '/items/DGBJA7A900B6502/000001_1614764722.jpg', 'name': 'SONY PS4 Pro主機CUH-7218系列 1TB-極致黑 盒損福利品', 'describe': '盒損福利全新機SONY PS4 Pro主機CUH-7218系列 1TB-極致黑\\\\r\\\\n僅盒損，內容物全新\\r\\n■滿足追求更高層次遊戲體驗的玩家\\r\\n■超高解析度，如臨其境震撼視覺享受\\r\\n■gpu性能升級，更細緻流暢、穩定的畫面\\r\\n■支援hdr，更逼真生動，如真實世界般視覺效果\\r\\n■外型三段傾斜設計具強烈存在感，中央點綴鏡面logo\\r\\n原廠一年保固，產品保固採線上登記', 'price': 9980, 'originPrice': 9980, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': ['C024813', 'C025307', 'C025308', 'C025306'], 'BU': 'ec'}]}data.keys()\n#> dict_keys(['QTime', 'totalRows', 'totalPage', 'range', 'cateName', 'q', 'subq', 'token', 'isMust', 'prods'])data[\"prods\"][:2]\n#> [{'Id': 'DEBB55-A900AXCHG', 'cateId': 'DEBB55', 'picS': '/items/DEBB55A900AXCHG/000002_1642059796.jpg', 'picB': '/items/DEBB55A900AXCHG/000001_1642059795.jpg', 'name': 'PX大通HR7PRO星光夜視行車紀錄器SONY感光元件GPS區間測速記錄器', 'describe': '每日強檔▼瘋殺特賣PX大通HR7PRO星光夜視旗艦王汽車行車紀錄器真HDR高動態SONY STARVIS感光元件GPS區間測速記錄器贈32G記憶卡\\\\r\\\\n《只有一天★限時狂降》\\r\\n開始﹕０１／１３(星期四)１０：００\\r\\n結束﹕０１／１４(星期五)１０：００\\r\\n網路價$4988．限時價↘$４１８８\\r\\n滿＄３０００再送３００ｐ幣\\r\\n======================================\\r\\n\\r\\n🚩【夜視新標竿】: hdr高動態錄影 +sony starvis星光級感光元件。\\r\\n>>真hdr:抗led或hid高強光、車牌影像清晰不過曝 starvis:提升整體亮度、解決環境過暗                \\r\\n🚩【安全新標竿】: ts碼流存檔機制。\\r\\n>>以秒截檔設計，確保錄影檔案不毀損  \\r\\n🚩【好用新標竿】: 一鍵進格式化。\\r\\n>>動動手指，輕鬆格式化    \\r\\n🚩【測速新標竿】: gps區間+定點測速雙警示。\\r\\n>>gps雙衛星，雙預警加倍安心\\r\\n\\r\\n👍👍👍多位論壇部落客一致推薦👍👍👍\\r\\n★pchome開箱focus專欄★大力推薦\\r\\n★tony的玩樂生活筆記★部落客開箱推薦\\r\\n★小老婆汽機車資訊網★網路車類媒體大力推薦\\r\\n★正宅爸3c開箱★部落客開箱推薦\\r\\n★星星陳世章★部落客開箱推薦\\r\\n★寶井秀人hyde★部落客開箱推薦\\r\\n★basic施鈞程★部落客開箱推薦\\r\\n★硬是要學★部落客開箱推薦\\r\\n★鑫部落★部落客開箱推薦\\r\\n★另有其他規格款式可選↓↓↓\\r\\n\\r\\n《★鑽石版頂規 ★ 》\\r\\n《★hr7 pro》\\r\\n《★鈦金版高規 ★ 》\\r\\n《★hr7g》\\r\\n《★白金版均規 ★ 》\\r\\n《★hr7》\\r\\n\\r\\n※六大優勢\\r\\n1. hdr高動態錄影+sony starvis星光夜視技術>>抗led強光、夜間車牌清晰\\r\\n2. ts碼流存檔機制>>以秒截檔設計，不正常斷電、車禍撞擊，錄影檔案不毀損\\r\\n3. gps區間測速>>警示功能提升\\r\\n4. 2吋ips高清螢幕>>廣視角，資訊顯示清楚\\r\\n5. 不掉落魔法貼>>全新支架設計，機身更穩固\\r\\n6. 一鍵進格式化>>簡易操作，確保錄影成功率\\r\\n\\r\\n※完整特色\\r\\n▼ 1080p 30fps hdr高動態\\r\\n▼ sony starvis超級星光夜視\\r\\n▼ f 1.4超大光圈+7玻1ir鏡頭\\r\\n▼ 1080p 60fps wdr寬動態\\r\\n▼ ips廣視角高清螢幕\\r\\n▼ 以秒截檔ts碼流存檔機制\\r\\n▼ 行車魔法貼不掉落支架\\r\\n▼ 148度大廣角\\r\\n▼ hud抬頭顯示\\r\\n▼ 13段曝光值調整\\r\\n▼ gps雙衛星接收快速定位\\r\\n▼ gps區間+定點測速照相雙預警\\r\\n▼ g-sensor碰撞自動偵測鎖檔\\r\\n▼ 一鍵鎖檔備份關鍵檔案 \\r\\n▼ 一鍵快捷啟動記憶卡格式化\\r\\n▼ 一鍵快速拍照\\r\\n▼ 記憶卡異常提醒\\r\\n▼ 記憶卡30天定期格式化提醒', 'price': 4188, 'originPrice': 4188, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': [], 'BU': 'ec'}, {'Id': 'DGBJGB-A900CAR5E', 'cateId': 'DGBJGB', 'picS': '/items/DGBJGBA900CAR5E/000002_1641459421.jpg', 'picB': '/items/DGBJGBA900CAR5E/000001_1641459421.jpg', 'name': 'SONY PS5原廠 DualSense 無線控制器-銀河紫 CFI-ZCT1G04', 'describe': 'SONY PS5原廠 DualSense 無線控制器銀河紫 CFI-ZCT1G04\\\\r\\\\n來自銀河系的新色 為遊戲之夜點燃熱情 \\r\\n創新ps5控制器 深入栩栩如生遊戲體驗 \\r\\n■搭載觸覺回饋和自適應扳機，更身歷其境的感受\\r\\n■直覺配置搭配強化操作桿，讓你掌握全局\\r\\n■觸覺回饋真實感受到武器後座力等動態震動\\r\\n■自適應扳機體驗往後拉弓、急剎車等力量與張力\\r\\n■內建麥克風與3.5mm耳機連接端\\r\\n■帶來更多方式向全世界播送冒險經歷\\r\\n\\r\\n☛看看其他銀河系新色-星塵紅', 'price': 2280, 'originPrice': 2280, 'author': '', 'brand': '', 'publishDate': '', 'sellerId': '', 'isPChome': 1, 'isNC17': 0, 'couponActid': ['C025307', 'C025308'], 'BU': 'ec'}]pd.DataFrame(data[\"prods\"]).head()\n#>                  Id  cateId  ...         couponActid  BU\n#> 0  DEBB55-A900AXCHG  DEBB55  ...                  []  ec\n#> 1  DGBJGB-A900CAR5E  DGBJGB  ...  [C025307, C025308]  ec\n#> 2  DMAAEC-A900BFLRR  DMAAEC  ...                  []  ec\n#> 3  DPAD06-A900BFGWY  DPADZ9  ...                  []  ec\n#> 4  DYAQBC-A900BNLZQ  DYAQF4  ...                  []  ec\n#> \n#> [5 rows x 16 columns]"},{"path":"json-apis.html","id":"get-data-from-apis","chapter":"4 JSON & APIs","heading":"4.2.1 Get data from APIs","text":"從這邊以下的資訊，就先不整理了，因為範例我都無法reproduce(因為認證的key已經不能用了)我計畫之後在爬蟲的主題(緯育+Yotta)，把這些東西處理好，畢竟爬蟲時最容易碰到這些問題","code":""},{"path":"json-apis.html","id":"set-api-parameters","chapter":"4 JSON & APIs","heading":"4.2.2 Set API parameters","text":"","code":""},{"path":"json-apis.html","id":"set-request-headers","chapter":"4 JSON & APIs","heading":"4.2.3 Set request headers","text":"","code":""},{"path":"json-apis.html","id":"working-with-nested-jsons","chapter":"4 JSON & APIs","heading":"4.3 Working with nested JSON’s","text":"","code":""},{"path":"json-apis.html","id":"combining-multiple-datasets","chapter":"4 JSON & APIs","heading":"4.4 Combining multiple datasets","text":"","code":""},{"path":"numpy-basics.html","id":"numpy-basics","chapter":"5 numpy basics","heading":"5 numpy basics","text":"","code":""},{"path":"numpy-basics.html","id":"why-numpy","chapter":"5 numpy basics","heading":"5.1 Why numpy","text":"numpy 是 numeric python 的縮寫，簡單來講，就是要來做數學運算的意思之前學的 list 是無法做數學計算的，所以在 numpy 裡，會把 list 轉成 numpy array 這種型別，就可以做計算了。看個例子：轉成 numpy array 就可以做計算了再看個例子","code":"height = [1.73, 1.68, 1.71, 1.89, 1.79]\nweight = [65.4, 59.2, 63.6, 88.4, 68.7]\nweight/(height**2)\n#> Error in py_call_impl(callable, dots$args, dots$keywords): TypeError: unsupported operand type(s) for ** or pow(): 'list' and 'int'\n#> \n#> Detailed traceback:\n#>   File \"<string>\", line 1, in <module>import numpy as np\nheight = np.array([1.73, 1.68, 1.71, 1.89, 1.79])\nweight = np.array([65.4, 59.2, 63.6, 88.4, 68.7])\nweight/(height**2)\n#> array([21.85171573, 20.97505669, 21.75028214, 24.7473475 , 21.44127836])python_list = [1,2,3]\nprint(python_list + python_list)\n#> [1, 2, 3, 1, 2, 3]\nnumpy_array = np.array([1,2,3])\nprint(numpy_array + numpy_array)\n#> [2 4 6]"},{"path":"numpy-basics.html","id":"多層-list-的介紹","chapter":"5 numpy basics","heading":"5.2 多層 list 的介紹","text":"從剛剛的介紹中，我們知道把 list 轉成 ndarray 後，就可以做數學運算。但剛剛舉的例子都很簡單，比較像是我找10筆數據，存進 list 這個 collection 裡面。但在 python 的 data science 應用中，我們可以收到更多不同類型的數據。例如：\n1張灰階影像的數據，是個矩陣型資料，如何存到 list 中？\n1張彩色影像數據，會是RGB 3 個通道，每個通道都是矩陣型資料，這要如何存到 list 中？\n10 張彩色影像數據，如何存到 list 中？\n5批資料，每批都有 10 張彩色影像，這樣的數據要如何存到 list 中？\n3個年份，每個年份都有5批資料，每批都有 10 張彩色影像，這樣的數據要如何存到 list 中？\n1張灰階影像的數據，是個矩陣型資料，如何存到 list 中？1張彩色影像數據，會是RGB 3 個通道，每個通道都是矩陣型資料，這要如何存到 list 中？10 張彩色影像數據，如何存到 list 中？5批資料，每批都有 10 張彩色影像，這樣的數據要如何存到 list 中？3個年份，每個年份都有5批資料，每批都有 10 張彩色影像，這樣的數據要如何存到 list 中？這些問題可以一直問下去，但大概已經可以發現，我只要用一層又一層的list來儲存，就可以搞定了。","code":""},{"path":"numpy-basics.html","id":"層-list","chapter":"5 numpy basics","heading":"5.2.1 2 層 list","text":"對於1張灰階影像資料，例如是這樣的一張矩陣型資料: \\(\\left[\\begin{array}{cc} 0 & 1\\\\1 & 0 \\\\1 & 1\\end{array}\\right]\\)，可以用數學寫成：\\[\n\\boldsymbol{X} \\R^{3 \\times 2}\n\\]在 python 中，會用這樣的 list 來儲存他：我們總是會很想用矩陣的角度去看他，但拜託你忍一忍，不要這樣做。因為之後要一路推廣下去。所以，我們現在改成用層次的方式來理解他：\\(R^{3 \\times 2}\\) 就讀成: 總共有3列，每一列都有2筆數據。那他的階層就會長成：\n第一列: [0, 1]\n第一列的第一個 element: 0\n第一列的第二個 element: 1\n\n第二列: [1, 0]\n第二列的第一個 element: 1\n第二列的第二個 element: 0\n\n第三列: [1, 1]\n第三列的第一個 element: 1\n第三列的第二個 element: 1\n\n第一列: [0, 1]\n第一列的第一個 element: 0\n第一列的第二個 element: 1\n第一列的第一個 element: 0第一列的第二個 element: 1第二列: [1, 0]\n第二列的第一個 element: 1\n第二列的第二個 element: 0\n第二列的第一個 element: 1第二列的第二個 element: 0第三列: [1, 1]\n第三列的第一個 element: 1\n第三列的第二個 element: 1\n第三列的第一個 element: 1第三列的第二個 element: 1也就是第一層是 \\(R^{3 \\times 2}\\) 的 3，第二層是 \\(R^{3 \\times 2}\\) 的 2所以，我們要練習，這樣寫 list：接著，來定義一些名詞： \\(R^{3 \\times 2}\\)，R的上面有2個數字相乘，我們稱它為2階張量，儲存的資料類型是 2d array。也就是說，這個張量的維度是2。然後 R 上面的長相是 \\(3 \\times 2\\)，所以我們說他的 shape 是 (3,2)我們來看一下這個 numpy array 的 attribute，就可以驗證上面講的內容：ndim 是 2，就表示 ndarray 是 2d array(n=2, 有兩層，R上面有2個數字相乘)shape 是 (3,2)，表示他是 \\(R^{3 \\times 2}\\) 的張量","code":"a = [\n  [0, 1], \n  [1, 0], \n  [1, 1]\n]\na = np.array(a)\na\n#> array([[0, 1],\n#>        [1, 0],\n#>        [1, 1]])# 第一步，先寫出第一層，有3列： a = [[], [], []] \n# 第二步，再把第二層的內容補進去，各2個element： a = [[0, 1], [1, 0], [1, 1]] a.ndim\n#> 2a.shape\n#> (3, 2)"},{"path":"numpy-basics.html","id":"層-list-1","chapter":"5 numpy basics","heading":"5.2.2 3 層 list","text":"對於1張彩色影像資料，他會有3張矩陣型資料，例如長成這樣：\\[\n\\left[\n\\left[\\begin{array}{cc} 0 & 1\\\\1 & 0 \\\\1 & 1\\end{array}\\right],\n\\left[\\begin{array}{cc} 0 & 0\\\\1 & 1 \\\\1 & 0\\end{array}\\right],\n\\left[\\begin{array}{cc} 1 & 1\\\\0 & 0 \\\\0 & 1\\end{array}\\right]\n\\right]\n\\]那我可以寫成這樣：\\(\\boldsymbol{X} \\R^{3 \\times 3 \\times 2}\\)\\[\n\\boldsymbol{X} = \\left[\nR^{3 \\times 2},\nG^{3 \\times 2},\nB^{3 \\times 2}\n\\right]\n\\]\n* 由 \\(R^{3 \\times 3 \\times 2}\\) 已可知道，他是 3d array(所以要給他3層)。shape是 3*3*2，所以第一層有3個 element，第二層有3個element，第三層有2個element。\n* 那我再造 list 時，第一步就是先寫第一層：然後第二層：最後，做出第三層：驗證一下，這個 \\(R^{3 \\times 3 \\times 2}\\) 是 3d array(因為R上面有3個數字相乘，或說，建立list的時候要寫到第3層)。shape是 3*3*2","code":"a = [\n  [],\n  [],\n  []\n]a = [\n  [\n    [],\n    [], \n    []\n  ],\n  [\n    [],\n    [], \n    []\n  ],\n  [\n    [],\n    [], \n    []\n  ]\n]a = [\n  [\n    [0, 1],\n    [1, 0], \n    [1, 1]\n  ],\n  [\n    [0, 0],\n    [1, 1], \n    [1, 0]\n  ],\n  [\n    [1, 1],\n    [0, 0], \n    [0, 1]\n  ]\n]\na = np.array(a)\na\n#> array([[[0, 1],\n#>         [1, 0],\n#>         [1, 1]],\n#> \n#>        [[0, 0],\n#>         [1, 1],\n#>         [1, 0]],\n#> \n#>        [[1, 1],\n#>         [0, 0],\n#>         [0, 1]]])print(f\"the dim of a is {a.ndim}\")\n#> the dim of a is 3\nprint(f\"the shape of a is {a.shape}\")\n#> the shape of a is (3, 3, 2)"},{"path":"numpy-basics.html","id":"層-list-2","chapter":"5 numpy basics","heading":"5.2.3 4 層 list","text":"剛剛介紹完，1張彩色影像資料要如何儲存。那如果 2 張彩色影像數據，要如何存到 list 中？很簡單嘛，現在變成是一個 \\(R^{2張 \\times 3通道 \\times 3列 \\times 2行}\\) 的資料，所以我要做一個 4D array(因為 R 上面有4個數字相乘，list要做到4層)，然後他的 shape 會是 (2,3,3,2)開始造 list ，第一步就是先寫第一層(2張圖片)：然後第二層，每張圖片，都有RGB三個通道：然後，第三層，每個 RGB 中，都有三列：最後，每一列裡面，都有兩個 element:驗證一下，這個 \\(R^{2張 \\times 3通道 \\times 3列 \\times 2行}\\)是 4d array(因為R上面有4個數字相乘，或說，建立list的時候要寫到第4層)。shape是 2*3*3*2","code":"a = [\n  [],\n  []\n]a = [\n  [\n    [],\n    [], \n    []\n  ],\n  [\n    [],\n    [], \n    []\n  ]\n]a = [\n  [\n    [\n      [],\n      [],\n      []\n    ],\n    [\n      [],\n      [],\n      []\n    ], \n    [\n      [],\n      [],\n      []\n    ]\n  ],\n  [\n    [\n      [],\n      [],\n      []\n    ],\n    [\n      [],\n      [],\n      []\n    ], \n    [\n      [],\n      [],\n      []\n    ]\n  ]\n]a = [\n  [\n    [\n      [0, 1],\n      [1, 0], \n      [1, 1]\n    ],\n    [\n      [0, 0],\n      [1, 1], \n      [1, 0]\n    ], \n    [\n      [1, 1],\n      [0, 0], \n      [0, 1]\n    ]\n  ],\n  [\n    [\n      [0, 0],\n      [1, 0], \n      [0, 1]\n    ],\n    [\n      [1, 1],\n      [1, 1], \n      [1, 1]\n    ], \n    [\n      [0, 0],\n      [0, 1], \n      [1, 0]\n    ]\n  ]\n]\na = np.array(a)\na\n#> array([[[[0, 1],\n#>          [1, 0],\n#>          [1, 1]],\n#> \n#>         [[0, 0],\n#>          [1, 1],\n#>          [1, 0]],\n#> \n#>         [[1, 1],\n#>          [0, 0],\n#>          [0, 1]]],\n#> \n#> \n#>        [[[0, 0],\n#>          [1, 0],\n#>          [0, 1]],\n#> \n#>         [[1, 1],\n#>          [1, 1],\n#>          [1, 1]],\n#> \n#>         [[0, 0],\n#>          [0, 1],\n#>          [1, 0]]]])print(f\"the dim of a is {a.ndim}\")\n#> the dim of a is 4\nprint(f\"the shape of a is {a.shape}\")\n#> the shape of a is (2, 3, 3, 2)\nprint(a.size)\n#> 36\nlen(a)\n#> 2"},{"path":"numpy-basics.html","id":"ndarray-的-dim-shape-與-axis","chapter":"5 numpy basics","heading":"5.3 ndarray 的 dim, shape, 與 axis","text":"在 ndarray 中，這三個詞很容易搞混，但又超重要，所以在這裡好好整理一下在 ndarray 中，這三個詞很容易搞混，但又超重要，所以在這裡好好整理一下講到維度，就會想到以前線性代數學到的東西：維度是指向量空間裡基底的數量。簡單的判斷方法，就是把 \\(R\\) 上面的數字乘出來就對了。講到維度，就會想到以前線性代數學到的東西：維度是指向量空間裡基底的數量。簡單的判斷方法，就是把 \\(R\\) 上面的數字乘出來就對了。例如 \\(R^3\\) 表示3維，意思是這個歐式空間中，需要3個基底向量(e.g. \\(\\left[\\begin{array}{cc} 1 \\\\0 \\\\ 0\\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 \\\\1 \\\\ 0\\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 \\\\0 \\\\ 1\\end{array}\\right]\\))，才能span出這個空間。所以我們稱它為 3 維(空間)，這個空間裡的每個點，就是一個3維向量。例如 \\(R^3\\) 表示3維，意思是這個歐式空間中，需要3個基底向量(e.g. \\(\\left[\\begin{array}{cc} 1 \\\\0 \\\\ 0\\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 \\\\1 \\\\ 0\\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 \\\\0 \\\\ 1\\end{array}\\right]\\))，才能span出這個空間。所以我們稱它為 3 維(空間)，這個空間裡的每個點，就是一個3維向量。又例如 \\(R^{2 \\times 2}\\) 是 \\(2 \\times 2\\) = 4維，意思是這個矩陣空間，需要 4 個基底向量(e.g. \\(\\left[\\begin{array}{cc} 1 & 0\\\\0 & 0 \\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 & 1\\\\0 & 0 \\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 & 0\\\\1 & 0 \\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 & 0\\\\0 & 1 \\end{array}\\right]\\))才能span出這個空間，所以我們稱它為 4 維，然後空間中的每個點都是 \\(2 \\times 2\\) 的矩陣。又例如 \\(R^{2 \\times 2}\\) 是 \\(2 \\times 2\\) = 4維，意思是這個矩陣空間，需要 4 個基底向量(e.g. \\(\\left[\\begin{array}{cc} 1 & 0\\\\0 & 0 \\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 & 1\\\\0 & 0 \\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 & 0\\\\1 & 0 \\end{array}\\right]\\), \\(\\left[\\begin{array}{cc} 0 & 0\\\\0 & 1 \\end{array}\\right]\\))才能span出這個空間，所以我們稱它為 4 維，然後空間中的每個點都是 \\(2 \\times 2\\) 的矩陣。對應到 R 的資料結構，就是：\n純量 (\\(R\\)), 對應到 numeric 的資料結構，例如 = 3\nn維向量 (\\(R^n\\)), 對應到 n個element的vector 的資料結構，例如 = c(1,2,3) 為 \\(R^3\\)，三維向量\nm n 矩陣(\\(R^{m \\times n}\\)), 對應到 m*n個 element 的 matrix 的資料結構，例如 = matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE) 為 \\(R^{2 \\times 3}\\)\n對應到 R 的資料結構，就是：純量 (\\(R\\)), 對應到 numeric 的資料結構，例如 = 3n維向量 (\\(R^n\\)), 對應到 n個element的vector 的資料結構，例如 = c(1,2,3) 為 \\(R^3\\)，三維向量m n 矩陣(\\(R^{m \\times n}\\)), 對應到 m*n個 element 的 matrix 的資料結構，例如 = matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE) 為 \\(R^{2 \\times 3}\\)那以前只學到矩陣，所以只學到 \\(R^{m \\times n}\\)，那有沒有想過， \\(R^{j \\times m \\times n}\\) 這是什麼東西？甚至，\\(R^{\\times j \\times m \\times n}\\) 這是什麼東西？那以前只學到矩陣，所以只學到 \\(R^{m \\times n}\\)，那有沒有想過， \\(R^{j \\times m \\times n}\\) 這是什麼東西？甚至，\\(R^{\\times j \\times m \\times n}\\) 這是什麼東西？簡單講，這種東西，就叫張量(tensor)。\\(R\\)的上面，如果有 n 個數字相乘，我們就叫他 n 階張量。例如：\n\\(R\\): 0階張量，就是以前學的純量\n\\(R^3\\): 1階張量(因為只有一個數字相乘)，就是以前學的向量\n\\(R^{2 \\times 2}\\): 2階張量(因為有2個數字相乘)，就是以前學的矩陣。對應到資料，就像是灰階影像，長寬各2個pixel。\n\\(R^{3 \\times 2 \\times 2}\\): 3階張量(因為有3個數字相乘)，這以前就沒學過了。對應到的資料，就像是彩色影像，3個通道(RGB)，每個通道都是長寬各2個pixel。\n\\(R^{5 \\times 3 \\times 2 \\times 2}\\): 4階張量(因為有4個數字相乘)。對應到的資料，就是有5張彩色影像。每張彩色影像，都有3個通道(RGB)，每個通道，都是長寬各2個pixel。\n簡單講，這種東西，就叫張量(tensor)。\\(R\\)的上面，如果有 n 個數字相乘，我們就叫他 n 階張量。例如：\\(R\\): 0階張量，就是以前學的純量\\(R^3\\): 1階張量(因為只有一個數字相乘)，就是以前學的向量\\(R^{2 \\times 2}\\): 2階張量(因為有2個數字相乘)，就是以前學的矩陣。對應到資料，就像是灰階影像，長寬各2個pixel。\\(R^{3 \\times 2 \\times 2}\\): 3階張量(因為有3個數字相乘)，這以前就沒學過了。對應到的資料，就像是彩色影像，3個通道(RGB)，每個通道都是長寬各2個pixel。\\(R^{5 \\times 3 \\times 2 \\times 2}\\): 4階張量(因為有4個數字相乘)。對應到的資料，就是有5張彩色影像。每張彩色影像，都有3個通道(RGB)，每個通道，都是長寬各2個pixel。那這個時候，我們講維度，就是在講 張量 的維度。那這個時候，我們講維度，就是在講 張量 的維度。所以，以前學的維度，在描述的對象，都是 向量空間 。所以，以前學的維度，在描述的對象，都是 向量空間 。那接下來要講的維度，他要描述的對象，是資料結構裡的陣列(array)，在python中叫他list，在deep learning中叫他tensor(張量)。都是同義詞。我們 focus 在 list 就好。那接下來要講的維度，他要描述的對象，是資料結構裡的陣列(array)，在python中叫他list，在deep learning中叫他tensor(張量)。都是同義詞。我們 focus 在 list 就好。但在 Python 世界，就直接推廣到 張量(tensor) 的概念。而張量對應的資料結構，就是 ndarray(n-dim array, n階張量)但在 Python 世界，就直接推廣到 張量(tensor) 的概念。而張量對應的資料結構，就是 ndarray(n-dim array, n階張量)重點來了，張量是啥？n-dim 的 dim 又是啥？ 哈哈笑死，這邊的 dim(維度) 根本就和以前線性代數學的不一樣，不要執著以前學過的定義，這樣會卡死。重點來了，張量是啥？n-dim 的 dim 又是啥？ 哈哈笑死，這邊的 dim(維度) 根本就和以前線性代數學的不一樣，不要執著以前學過的定義，這樣會卡死。腦袋轉一下，直接來學新的系統。n-d array 的 d，是指 $R$ 上面有 d 個數字相乘，所以他的維度d (d-dimension)，就是指R上面有d個數字相乘。對應到list的資料結構，就是有d個階層。以下逐一介紹：腦袋轉一下，直接來學新的系統。n-d array 的 d，是指 $R$ 上面有 d 個數字相乘，所以他的維度d (d-dimension)，就是指R上面有d個數字相乘。對應到list的資料結構，就是有d個階層。以下逐一介紹：","code":""},{"path":"numpy-basics.html","id":"d-array-0-dim-array-0階張量-0層的資料結構","chapter":"5 numpy basics","heading":"5.3.1 0D array = 0-dim array = 0階張量 = 0層的資料結構","text":"例如純量3， \\(3 \\R\\)，R上面有0個數字相乘，所以他叫 0D array。來看一下在 python 的資料結構會長怎樣：我們可以用 .ndim 這個屬性，來看他的dim，也就是幾D array:很明顯的，告訴我他是個 0D array然後用 .shape，來看他的 shape是空的，也就是 \\(R^{()}\\) 這樣的意思，裡面就是沒東西相乘","code":"a = np.array(3)\nprint(a)\n#> 3\nprint(type(a))\n#> <class 'numpy.ndarray'>a.ndim\n#> 0a.shape\n#> ()"},{"path":"numpy-basics.html","id":"階張量-1d-array","chapter":"5 numpy basics","heading":"5.3.2 1 階張量 (1D array)","text":"1 階張量，就是 \\(R\\)的上面，有 1 個數字相乘。相乘的樣貌就叫shape舉個例子，\\(\\left[\\begin{array}{cc} 2 \\\\3 \\\\ 4\\end{array}\\right] \\R^{3}\\)，所以這樣的資料，應該是 1D array, 然後 shape = 3在 python 的資料結構中，我就是用 1 層的list來處理：","code":"a = np.array([2,3,4])\nprint(a)\n#> [2 3 4]\nprint(type(a))\n#> <class 'numpy.ndarray'>\nprint(f\"the dim of a is {a.ndim}\")\n#> the dim of a is 1\nprint(f\"the shape of a is {a.shape}\")\n#> the shape of a is (3,)"},{"path":"numpy-basics.html","id":"階張量-2d-array","chapter":"5 numpy basics","heading":"5.3.3 2 階張量 (2D array)","text":"灰階影像的數據，就都是長這樣。舉例來說，我拿到的影像，長寬都只有2個像素，所以長成\\(\\left[\\begin{array}{cc} 1 & 2\\\\3 & 4 \\end{array}\\right] \\R^{2 \\times 2}\\)那\\(R\\)的上面，有 2 個數字相乘，所以是 2D array。相乘的樣貌是2*2，所以shape為(2,2)那這種2D array，在python就是存成 2-dim array，也就是 2 層的list：從這邊開始，要特別注意 2D array 就是 2層list結構。可以這樣思考。:\n[,]\n1\n2\n\n[,]\n3\n4\n\n[,]\n1\n2\n12[,]\n3\n4\n34所以由左到右看，第一層有2個 element，第二層也是2個element。第一層我們又叫他第0軸(axis = 0)，第二層我們又叫他第1軸(axis=1)。","code":"# 2 層的結構  \na = [\n  [1,2],\n  [3,4]\n]\na = np.array(a)\nprint(a)\n#> [[1 2]\n#>  [3 4]]\nprint(type(a))\n#> <class 'numpy.ndarray'>\nprint(f\"the dim of a is {a.ndim}\")\n#> the dim of a is 2\nprint(f\"the shape of a is {a.shape}\")\n#> the shape of a is (2, 2)len(a)\n#> 2"},{"path":"numpy-basics.html","id":"階張量-3d-array","chapter":"5 numpy basics","heading":"5.3.4 3 階張量 (3D array)","text":"","code":""},{"path":"numpy-basics.html","id":"階張量-4d-array","chapter":"5 numpy basics","heading":"5.3.5 4 階張量 (4D array)","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"pandas_cheatsheet","chapter":"6 pandas_cheatsheet","heading":"6 pandas_cheatsheet","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"overview","chapter":"6 pandas_cheatsheet","heading":"6.1 overview","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"取colnames-rownames-values","chapter":"6 pandas_cheatsheet","heading":"6.2 取colnames, rownames, values","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"loc-.iloc","chapter":"6 pandas_cheatsheet","heading":"6.3 .loc & .iloc","text":"","code":"錯誤: df.loc[[列名1, 列名2], :]\n\n正確: df.loc[[列名1, 列名2]]錯誤: df.loc[[\"列名2\":\"列名7\"]]\n\n正確: df.loc[\"列名2\":\"列名7\"]錯誤: df.loc[:, [\"行名2\":\"行名7\"]]\n\n正確: df.loc[:, \"行名2\":\"行名7\"]df.iloc[0,2] # 返回value\ndf.iloc[[0],[2]] # 返回df type\ndf.iloc[[0,2], [0,1]] #多列多行\ndf.iloc[[0,2]] # 多列\ndf.iloc[:, [0,1]] # 多行"},{"path":"pandas_cheatsheet.html","id":"indexing-another-filter-arrange-method","chapter":"6 pandas_cheatsheet","heading":"6.4 indexing (another filter & arrange method)","text":"簡單講，就是把某個column，設成row index，那接下來就可以用.loc[[列名1, 列名2]] 來進行filter簡單講，就是把某個column，設成row index，那接下來就可以用.loc[[列名1, 列名2]] 來進行filter更特別的是，row index可以接受multi-column，所以當我們想篩選出類似實驗設計的多種treatment時(例如”=1且B=2” 或 “=2且B=3”)，特別適合用這種方式來篩選。更特別的是，row index可以接受multi-column，所以當我們想篩選出類似實驗設計的多種treatment時(例如”=1且B=2” 或 “=2且B=3”)，特別適合用這種方式來篩選。將欄位 設成 index將 index 回覆成欄位df_ind = df.set_index(“”)df_orig = df_ind.reset_index()","code":"df %>%\n  filter(a == 1)df \\\n  .set_index(\"a\") \\\n  .loc[\"1\"] \\\n  .reset_index()df %>%\n  filter(a %in% c(\"1\",\"2\"))df \\\n  .set_index(\"a\") \\\n  .loc[[\"1\",\"2\"]] \\\n  .reset_index()df %>%\n  filter(\n    (a == 1 & b == 2) |\n    (a == 3 & b == 5)\n  )df \\\n  .set_index(\"a\",\"b\") \\\n  .loc[(1,2), (3,5)] \\\n  .reset_index()df %>%\n  arrange(a)df \\\n  .set_index(\"a\") \\\n  .sort_index() \\\n  .reset_index()df %>%\n  arrange(desc(a))df \\\n  .set_index(\"a\") \\\n  .sort_index() \\\n  .reset_index(ascending = False)df %>%\n  arrange(a, desc(b))df \\\n  .set_index(\"a\", \"b\") \\\n  .sort_index(\n    level = [\"a\",\"b\"],\n    ascending = [True, False]\n  )"},{"path":"pandas_cheatsheet.html","id":"selectfilterarrangemutate","chapter":"6 pandas_cheatsheet","heading":"6.5 select/filter/arrange/mutate","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"select","chapter":"6 pandas_cheatsheet","heading":"6.5.1 select","text":"","code":"df %>%\n  select(col1)df[\"col1\"] # series\ndf[[\"col1\"]] # dfdf %>%\n  select(col1, col2)df[[\"col1\", \"col2\"]]df %>%\n  select(col1:col3)df.loc[:, \"col1\":\"col3\"]df[,1:3]df.iloc[:, 0:3] # from:stopn = ncol(df)\ndf[, (n-3):n]df.iloc[:, -3:]df %>%\n  select(where(is.numeric))df \\\n  .select_dtypes(\n    include = [\"int\",\"float\"]\n  )df %>%\n  select(where(!is.numeric))df \\\n  .select_dtypes(\n    exclude = [\"int\",\"float\"]\n  )df %>%\n  select(-c(col1, col2))df.drop([\"col1\", \"col2\"], axis =1)df %>%\n  select(-(col1:col3))df %>%\n  select(col3, everything())my_list = df.columns.tolist()\nmy_list.remove(\"col3\")\nnew_order = [\"col3\", *my_list]\ndf[new_order]df %>%\n  select(col3, col5, everything())my_list = df.columns.tolist()\nfront = [\"col3\", \"col5\"]\nend = [col for col in my_list if col not in front]\nnew_order = [*front, *end]\ndf[new_order]front = [\"col3\", \"col5\"]\ndf1 = df[front]\ndf2 = df.drop(front, axis = 1)\npd.concat([df1, df2], axis = 1)"},{"path":"pandas_cheatsheet.html","id":"filter","chapter":"6 pandas_cheatsheet","heading":"6.5.2 filter","text":"","code":"df %>%\n  filter(col1 == 1)df[df[\"col1\"]==1]\ndf.query(\"col1 == 1\")df %>%\n  filter(col1 > ymd(\"2021-01-01\"))df[df[\"col1\"] > pd.datetime(\"2021-01-01\")]\ndf.query(\"col1 > @pd.datetime(\"2021-01-01\")\")df %>%\n  filter(col1 %in% c(\"brown\", \"black\"))df[df[\"col1\"].isin([\"brown\",\"black\"])]df %>%\n  filter(! (col1 %in% c(\"brown\", \"black\")))df[-df[\"col1\"].isin([\"brown\",\"black\"])\n# python 的 - 就是R的 !df %>%\n  filter(col1 == 1, col2 == 1)df[df[\"col1\"]==1 & df[\"col2\"]==1]\n\ndf.query(\"col1 == 1 & col2 ==1\")df[1:5,]df.iloc[0:5,:]df[c(1,3,4), c(2,5)]df.iloc[[0,2,3], [1,4]]"},{"path":"pandas_cheatsheet.html","id":"arrange","chapter":"6 pandas_cheatsheet","heading":"6.5.3 arrange","text":"","code":"df %>%\n  arrange(col1)df \\\n  .sort_values(\"col1\")df %>%\n  arrange(desc(col1))df \\\n  .sort_values(\"col1\", ascending = False)df %>%\n  arrange(col1, col2)df \\\n  .sort_values([\"col1\", \"col2\"])df %>%\n  arrange(col1, desc(col2))df \\\n  .sort_values(\n    [\"col1\",\"col2\"],\n    ascending = [True, False]\n  )"},{"path":"pandas_cheatsheet.html","id":"mutate-and-rename","chapter":"6 pandas_cheatsheet","heading":"6.5.4 mutate and rename","text":"","code":"df %>%\n  mutate(c = a - b)df['c'] = df['a'] - df['b']\ndf.assign(c = df[\"a\"]-df[\"b\"])df %>%\n  filter(a == \"a1\") %>%\n  mutate(c = a - b)df.query(\"a == 'a1'\") \\\n  .assign(c = lambda x: x['a']-x['b'])\n# x 是 current data 的代名詞，因為這邊不是要用df['a']了df %>%\n  rename(col_new = col_old)df \\\n  .rename(\n    columns = {'col_old':'col_new'}\n  )"},{"path":"pandas_cheatsheet.html","id":"group_by-summarise","chapter":"6 pandas_cheatsheet","heading":"6.6 group_by & summarise","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"對-vectorseries-做-summarise-結果是數值","chapter":"6 pandas_cheatsheet","heading":"6.6.1 對 vector/series 做 summarise (結果是數值)","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"對-dataframe-做-summarise-結果仍是df","chapter":"6 pandas_cheatsheet","heading":"6.6.2 對 dataframe 做 summarise (結果仍是df)","text":"","code":"df %>%\n  summarise(mean_col1 = mean(col1)) #仍是 table# series\ndf[['col1']].mean()\n\n# dataframe\ndf[['col1']].mean()\n\ndf[['col1']].mean() \\\n  .to_frame(name = \"mean_col1\")\n\ndf[['col1']].agg(np.mean) \\\n  .to_frame(name = \"mean_col1\")\n\ndf \\\n  .agg({'col1': np.mean}) \\\n  .to_frame(name = \"mean_col1\")df %>%\n  summarise(\n    mean_col1 = mean(col1),\n    med_col1 = median(col1)\n  )df \\\n  .agg(\n    {\n      'col1': np.mean,\n      'col2': np.median\n    }\n  ) #沒辦法命名了，格式是直的df %>%\n  summarise(\n    mean(col1),\n    mean(col2),\n    median(col2)\n  )df \\\n  .agg({\n    'col1': np.mean,\n    'col2': [np.mean, np.median]\n  }) # 格式變成 crossrange2 = function(x){\n  max(x) - min(x)\n}\n\nsummarise(df, mean(col1), median(col2), range2(col1))def range2(x):\n  return x.max() - x.min()\n\ndf.agg([np.mean, np.median, range2])"},{"path":"pandas_cheatsheet.html","id":"對整張-table-的-各column-做-summarise","chapter":"6 pandas_cheatsheet","heading":"6.6.3 對整張 table 的 各column 做 summarise","text":"colSum(df) # length = p 的 vectorsummarise_all(df, sum) # 1xp tabledf.sum() # px1 seriesdf.agg(np.sum) # px1 seriesdf.sum().to_frame() # px1 tabledf.sum().to_frame().T # 1xp table","code":"df.agg([np.sum, np.median])df %>%\n  summarise(\n    across(c(col1, col2), sum)\n  )df[['col1', 'col2']] \\\n  .sum() # 2x1 series"},{"path":"pandas_cheatsheet.html","id":"group_by-summarise-1","chapter":"6 pandas_cheatsheet","heading":"6.6.4 group_by + summarise","text":"","code":"df %>%\n  group_by(col1) %>%\n  summarise(mean(col2))df.groupby(\"col1\")[\"col2\"].mean()\n\ndf.groupby(\"col1\")[\"col2\"].agg(np.mean)\n\ndf[[\"col1\",\"col2\"]].groupby(\"col1\").agg(np.mean)\n\ndf.groupby(\"col1\").agg({\"col2\": np.mean})df %>%\n  group_by(col1) %>%\n  summarise(mean(col2), median(col2))df.groupby(\"col1\")[\"col2\"].agg([np.mean, np.median])\n\ndf.groupby(\"col1\").agg({\"col2\": [np.mean, np.median]})df %>%\n  group_by(col1, col2) %>%\n  summarise(mean(col3),mean(col4), min(col3), min(col4))df.groupby([\"col1\",\"col2\"])[\"col3\",\"col4\"].agg([np.mean, np.min)df %>%\n  group_by(col1, col2) %>%\n  summarise(mean(col3), median(col4))df.groupby([\"col1\",\"col2\"]).agg({'col3': 'mean', 'col4', 'median'})"},{"path":"pandas_cheatsheet.html","id":"group_by-mutate","chapter":"6 pandas_cheatsheet","heading":"6.6.5 group_by + mutate","text":"","code":"df %>%\n  group_by(a) %>%\n  mutate(b = c + d)df %>%\n  group_by(a) %>%\n  mutate(b = (b - mean(b))/sd(b))# 對 a 分組，normalize所有column\ndf \\\n  .groupby('a') \\\n  .apply(lambda x: (x - x.mean())/x.std()) \\\n  .reset_index()"},{"path":"pandas_cheatsheet.html","id":"group_by-filter","chapter":"6 pandas_cheatsheet","heading":"6.6.6 group_by + filter","text":"","code":"df %>%\n  group_by(a) %>%\n  filter(b > 50)df \\\n    .groupby(\"a\") \\\n    .apply(lambda x: x[x[\"b\"]>50])"},{"path":"pandas_cheatsheet.html","id":"group_by-arrange","chapter":"6 pandas_cheatsheet","heading":"6.6.7 group_by + arrange","text":"","code":"df %>%\n  group_by(a) %>%\n  arrange(b)"},{"path":"pandas_cheatsheet.html","id":"group_by-slice","chapter":"6 pandas_cheatsheet","heading":"6.6.8 group_by + slice","text":"","code":"df %>%\n  group_by(a) %>%\n  slice(1:5)df \\\n  .groupby(a) \\\n  .head(5)\n\ndf \\\n  .groupby(a) \\\n  .apply(lambda x: x.iloc[:5])"},{"path":"pandas_cheatsheet.html","id":"others","chapter":"6 pandas_cheatsheet","heading":"6.7 others","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"distinct-unique","chapter":"6 pandas_cheatsheet","heading":"6.7.1 distinct & unique","text":"","code":"df %>%\n  distinct(col1)# series\ndf[\"col1\"].drop_duplicates()\n# dataframe\ndf[[\"col1\"]].drop_duplicates()df %>%\n  distinct(col1, col2)df[[\"col1\", \"col2\"]].drop_duplicates()df['col1'].unique() # ndarray"},{"path":"pandas_cheatsheet.html","id":"slice","chapter":"6 pandas_cheatsheet","heading":"6.7.2 slice","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"count","chapter":"6 pandas_cheatsheet","heading":"6.7.3 count","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"largestsmallest","chapter":"6 pandas_cheatsheet","heading":"6.7.4 largest/smallest","text":"","code":"df.nlargest(\n  n = 5,\n  columns = 'total_price'\n)df.nsmallest(\n  n = 5,\n  columns = 'total_price'\n)"},{"path":"pandas_cheatsheet.html","id":"random-sampling-rows","chapter":"6 pandas_cheatsheet","heading":"6.7.5 random sampling rows","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"categorization","chapter":"6 pandas_cheatsheet","heading":"6.7.6 categorization","text":"","code":"pd.cut(df['price'], bins = 3)\n\npd.cut(df['price'], bins = 3, labels = ['low', 'medium', 'high'])pd.qcut(df['price'], q = [0, 0.25, 0.5, 0.75, 1])\n\npd.qcut(df['price'], q = 4) #4等分"},{"path":"pandas_cheatsheet.html","id":"missing-data","chapter":"6 pandas_cheatsheet","heading":"6.7.7 missing data","text":"","code":""},{"path":"pandas_cheatsheet.html","id":"pivot-table","chapter":"6 pandas_cheatsheet","heading":"6.8 pivot table","text":"pandas有個dplyr沒有的優點，就是他可以做excel的樞紐分析表最簡單的例子，就是你可以指定table的row是性別，col是年級，然後cell是去算該性別該年級下的平均成績之類的","code":"df.pivot_table(\n  index = cat_col1, #row\n  columns = cat_col2, #col\n  values = con_col, #cell要by誰做計算\n  aggfunc = np.mean, #計算用的function\n  margins = True #是否要算overall row/col\n)"},{"path":"pandas_cheatsheet.html","id":"字串相關的處理","chapter":"6 pandas_cheatsheet","heading":"6.9 字串相關的處理","text":"","code":"# 篩選出 a 欄位有 \"ist\" 字串的列\ndf %>%\n  mutate(a = str_to_lower(a)) %>%\n  filter(str_detect(a, \"ist\"))# 篩選出 a 欄位有 \"ist\" 字串的列\ndf[df['a'].str.lower().str.contains(\"ist\")]"},{"path":"pandas_cheatsheet.html","id":"離散化的處理","chapter":"6 pandas_cheatsheet","heading":"6.10 離散化的處理","text":"","code":"# 將數值變數a，切成三等分\ndf.assign(a_cat = pd.cut(df['price'], bins = 3, labels = ['low', 'medium','high'))# 將數值變數a，用我自定義的quantile，切成三等分\ndf.assign(a_cat = pd.qcut(df['price'], q = q=[0, 0.33, 0.66, 1]), labels = ['low', 'medium','high'))"},{"path":"pandas_cheatsheet.html","id":"combine-variables","chapter":"6 pandas_cheatsheet","heading":"6.11 Combine variables","text":"","code":"left_join(x, y,\n        by = c(\"col1_x\" = \"col1_y, \"col2_x\"=\"col2_y),\n        suffix = c(\"x\", \"y\"))x.merge(y,\n        how = \"left\",\n        left_on = [\"col1_x\",\"col2_x\",\n        right_on = [\"col1_y\",\"col2_y\"],\n        suffixes = [\"_x\",\"_y\"])"},{"path":"pandas_cheatsheet.html","id":"combine-cases","chapter":"6 pandas_cheatsheet","heading":"6.12 Combine cases","text":"","code":""},{"path":"pandas-basics.html","id":"pandas-basics","chapter":"7 Pandas basics","heading":"7 Pandas basics","text":"","code":"import pandas as pd"},{"path":"pandas-basics.html","id":"inspecting-dataframe","chapter":"7 Pandas basics","heading":"7.1 Inspecting DataFrame","text":"拿到資料的第一步，就是去看看這份資料裡有甚麼東西拿到資料的第一步，就是去看看這份資料裡有甚麼東西在R裡面，最常用的就是 head()來先看前幾筆，dim()來看一下列數和行數，str()來看各欄位的type，用summary來看各欄位的描述統計量在R裡面，最常用的就是 head()來先看前幾筆，dim()來看一下列數和行數，str()來看各欄位的type，用summary來看各欄位的描述統計量在Python也是一樣，只是R都是用function，但python是物件導向，所以是用methods\nR\nPython\nhead(df, n)\ndf.head(n)\ndim(df)\ndf.shape\nstr(df)\ndf.info()\nsummary(df)\ndf.describe()\n在Python也是一樣，只是R都是用function，但python是物件導向，所以是用methods接下來我們就來讀一筆資料，然後看看這些結果接下來我們就來讀一筆資料，然後看看這些結果這筆資料是在描述美國各州無家可歸的人數有多少第二個欄位和第一個欄位，就在講是哪個州的哪個地區第三個欄位的individuals在講the number homeless individuals part family children第四個欄位的family_members在講the number homeless individuals part family children第五個欄位是這個州的人口有多少這筆資料有51的rows，5個column","code":"homelessness = pd.read_csv(\"data/homelessness.csv\")homelessness.head()\n#>    Unnamed: 0              region  ... family_members  state_pop\n#> 0           0  East South Central  ...          864.0    4887681\n#> 1           1             Pacific  ...          582.0     735139\n#> 2           2            Mountain  ...         2606.0    7158024\n#> 3           3  West South Central  ...          432.0    3009733\n#> 4           4             Pacific  ...        20964.0   39461588\n#> \n#> [5 rows x 6 columns]homelessness.shape\n#> (51, 6)homelessness.info()\n#> <class 'pandas.core.frame.DataFrame'>\n#> RangeIndex: 51 entries, 0 to 50\n#> Data columns (total 6 columns):\n#>  #   Column          Non-Null Count  Dtype  \n#> ---  ------          --------------  -----  \n#>  0   Unnamed: 0      51 non-null     int64  \n#>  1   region          51 non-null     object \n#>  2   state           51 non-null     object \n#>  3   individuals     51 non-null     float64\n#>  4   family_members  51 non-null     float64\n#>  5   state_pop       51 non-null     int64  \n#> dtypes: float64(2), int64(2), object(2)\n#> memory usage: 2.5+ KBhomelessness.describe()\n#>        Unnamed: 0    individuals  family_members     state_pop\n#> count   51.000000      51.000000       51.000000  5.100000e+01\n#> mean    25.000000    7225.784314     3504.882353  6.405637e+06\n#> std     14.866069   15991.025083     7805.411811  7.327258e+06\n#> min      0.000000     434.000000       75.000000  5.776010e+05\n#> 25%     12.500000    1446.500000      592.000000  1.777414e+06\n#> 50%     25.000000    3082.000000     1482.000000  4.461153e+06\n#> 75%     37.500000    6781.500000     3196.000000  7.340946e+06\n#> max     50.000000  109008.000000    52070.000000  3.946159e+07"},{"path":"pandas-basics.html","id":"parts-of-a-dataframe","chapter":"7 Pandas basics","heading":"7.2 Parts of a DataFrame","text":"對data有初步認識後，我們常常就會想來擷取這份data的部分資訊對data有初步認識後，我們常常就會想來擷取這份data的部分資訊除了擷取某些column，某些row外，在R我們也常常用colnames()和rownames()，來擷取dataframe的column names & row names除了擷取某些column，某些row外，在R我們也常常用colnames()和rownames()，來擷取dataframe的column names & row names在Python，一樣，又是因為物件導向，所以他是用attribute(因為是這份df的特性，而不是methods，所以使用時，也不用加括號)來得到這些資訊\nR\nPython\ncolnames(data)\ndata.columns\nrownames(data)\ndata.index\n\ndata.values\n在Python，一樣，又是因為物件導向，所以他是用attribute(因為是這份df的特性，而不是methods，所以使用時，也不用加括號)來得到這些資訊最後，python比R多出來的東西，是value，他可以只取出DataFrame裡面的值。那此時的data type，就會是二維list:","code":"print(homelessness.columns)\n#> Index(['Unnamed: 0', 'region', 'state', 'individuals', 'family_members',\n#>        'state_pop'],\n#>       dtype='object')print(homelessness.index)\n#> RangeIndex(start=0, stop=51, step=1)homelessness.values\n#> array([[0, 'East South Central', 'Alabama', 2570.0, 864.0, 4887681],\n#>        [1, 'Pacific', 'Alaska', 1434.0, 582.0, 735139],\n#>        [2, 'Mountain', 'Arizona', 7259.0, 2606.0, 7158024],\n#>        [3, 'West South Central', 'Arkansas', 2280.0, 432.0, 3009733],\n#>        [4, 'Pacific', 'California', 109008.0, 20964.0, 39461588],\n#>        [5, 'Mountain', 'Colorado', 7607.0, 3250.0, 5691287],\n#>        [6, 'New England', 'Connecticut', 2280.0, 1696.0, 3571520],\n#>        [7, 'South Atlantic', 'Delaware', 708.0, 374.0, 965479],\n#>        [8, 'South Atlantic', 'District of Columbia', 3770.0, 3134.0,\n#>         701547],\n#>        [9, 'South Atlantic', 'Florida', 21443.0, 9587.0, 21244317],\n#>        [10, 'South Atlantic', 'Georgia', 6943.0, 2556.0, 10511131],\n#>        [11, 'Pacific', 'Hawaii', 4131.0, 2399.0, 1420593],\n#>        [12, 'Mountain', 'Idaho', 1297.0, 715.0, 1750536],\n#>        [13, 'East North Central', 'Illinois', 6752.0, 3891.0, 12723071],\n#>        [14, 'East North Central', 'Indiana', 3776.0, 1482.0, 6695497],\n#>        [15, 'West North Central', 'Iowa', 1711.0, 1038.0, 3148618],\n#>        [16, 'West North Central', 'Kansas', 1443.0, 773.0, 2911359],\n#>        [17, 'East South Central', 'Kentucky', 2735.0, 953.0, 4461153],\n#>        [18, 'West South Central', 'Louisiana', 2540.0, 519.0, 4659690],\n#>        [19, 'New England', 'Maine', 1450.0, 1066.0, 1339057],\n#>        [20, 'South Atlantic', 'Maryland', 4914.0, 2230.0, 6035802],\n#>        [21, 'New England', 'Massachusetts', 6811.0, 13257.0, 6882635],\n#>        [22, 'East North Central', 'Michigan', 5209.0, 3142.0, 9984072],\n#>        [23, 'West North Central', 'Minnesota', 3993.0, 3250.0, 5606249],\n#>        [24, 'East South Central', 'Mississippi', 1024.0, 328.0, 2981020],\n#>        [25, 'West North Central', 'Missouri', 3776.0, 2107.0, 6121623],\n#>        [26, 'Mountain', 'Montana', 983.0, 422.0, 1060665],\n#>        [27, 'West North Central', 'Nebraska', 1745.0, 676.0, 1925614],\n#>        [28, 'Mountain', 'Nevada', 7058.0, 486.0, 3027341],\n#>        [29, 'New England', 'New Hampshire', 835.0, 615.0, 1353465],\n#>        [30, 'Mid-Atlantic', 'New Jersey', 6048.0, 3350.0, 8886025],\n#>        [31, 'Mountain', 'New Mexico', 1949.0, 602.0, 2092741],\n#>        [32, 'Mid-Atlantic', 'New York', 39827.0, 52070.0, 19530351],\n#>        [33, 'South Atlantic', 'North Carolina', 6451.0, 2817.0, 10381615],\n#>        [34, 'West North Central', 'North Dakota', 467.0, 75.0, 758080],\n#>        [35, 'East North Central', 'Ohio', 6929.0, 3320.0, 11676341],\n#>        [36, 'West South Central', 'Oklahoma', 2823.0, 1048.0, 3940235],\n#>        [37, 'Pacific', 'Oregon', 11139.0, 3337.0, 4181886],\n#>        [38, 'Mid-Atlantic', 'Pennsylvania', 8163.0, 5349.0, 12800922],\n#>        [39, 'New England', 'Rhode Island', 747.0, 354.0, 1058287],\n#>        [40, 'South Atlantic', 'South Carolina', 3082.0, 851.0, 5084156],\n#>        [41, 'West North Central', 'South Dakota', 836.0, 323.0, 878698],\n#>        [42, 'East South Central', 'Tennessee', 6139.0, 1744.0, 6771631],\n#>        [43, 'West South Central', 'Texas', 19199.0, 6111.0, 28628666],\n#>        [44, 'Mountain', 'Utah', 1904.0, 972.0, 3153550],\n#>        [45, 'New England', 'Vermont', 780.0, 511.0, 624358],\n#>        [46, 'South Atlantic', 'Virginia', 3928.0, 2047.0, 8501286],\n#>        [47, 'Pacific', 'Washington', 16424.0, 5880.0, 7523869],\n#>        [48, 'South Atlantic', 'West Virginia', 1021.0, 222.0, 1804291],\n#>        [49, 'East North Central', 'Wisconsin', 2740.0, 2167.0, 5807406],\n#>        [50, 'Mountain', 'Wyoming', 434.0, 205.0, 577601]], dtype=object)type(homelessness.values)\n#> <class 'numpy.ndarray'>"},{"path":"pandas-basics.html","id":"select-filter-arrange-mutate","chapter":"7 Pandas basics","heading":"7.3 select, filter, arrange, & mutate","text":"在R裡面，我最常用到的四個函數，在pandas裡面都有對應的method可以用","code":""},{"path":"pandas-basics.html","id":"select-1","chapter":"7 Pandas basics","heading":"7.3.1 select","text":"來練習吧：我想先select “individual”這個欄位再來，我想select “state”和”family_members”這兩個column再來，我想select從individuals到family_members","code":"ind_series = homelessness[\"individuals\"]\nprint(type(ind_series))\n#> <class 'pandas.core.series.Series'>ind_series.head()\n#> 0      2570.0\n#> 1      1434.0\n#> 2      7259.0\n#> 3      2280.0\n#> 4    109008.0\n#> Name: individuals, dtype: float64ind_df = homelessness[[\"individuals\"]]\nprint(type(ind_df))\n#> <class 'pandas.core.frame.DataFrame'>print(ind_df.head())\n#>    individuals\n#> 0       2570.0\n#> 1       1434.0\n#> 2       7259.0\n#> 3       2280.0\n#> 4     109008.0state_fam = homelessness[[\"state\", \"family_members\"]]\nprint(state_fam.head())\n#>         state  family_members\n#> 0     Alabama           864.0\n#> 1      Alaska           582.0\n#> 2     Arizona          2606.0\n#> 3    Arkansas           432.0\n#> 4  California         20964.0ind_to_fam = homelessness.loc[:, \"individuals\":\"family_members\"]\nprint(ind_to_fam.head())\n#>    individuals  family_members\n#> 0       2570.0           864.0\n#> 1       1434.0           582.0\n#> 2       7259.0          2606.0\n#> 3       2280.0           432.0\n#> 4     109008.0         20964.0"},{"path":"pandas-basics.html","id":"filter-1","chapter":"7 Pandas basics","heading":"7.3.2 filter","text":"篩選出individuals > 10000的資料篩選出region = “Mountain”的資料篩選出family members < 1000 & region = “Pacific”篩選出region是”South Atlantic”或是”Mid_atlantic”","code":"homelessness.query(\"individuals > 10000\")\n#>     Unnamed: 0              region  ... family_members  state_pop\n#> 4            4             Pacific  ...        20964.0   39461588\n#> 9            9      South Atlantic  ...         9587.0   21244317\n#> 32          32        Mid-Atlantic  ...        52070.0   19530351\n#> 37          37             Pacific  ...         3337.0    4181886\n#> 43          43  West South Central  ...         6111.0   28628666\n#> 47          47             Pacific  ...         5880.0    7523869\n#> \n#> [6 rows x 6 columns]homelessness.query(\"region == 'Mountain'\").head()\n#>     Unnamed: 0    region     state  individuals  family_members  state_pop\n#> 2            2  Mountain   Arizona       7259.0          2606.0    7158024\n#> 5            5  Mountain  Colorado       7607.0          3250.0    5691287\n#> 12          12  Mountain     Idaho       1297.0           715.0    1750536\n#> 26          26  Mountain   Montana        983.0           422.0    1060665\n#> 28          28  Mountain    Nevada       7058.0           486.0    3027341homelessness.query(\"family_members < 1000 & region == 'Pacific'\")\n#>    Unnamed: 0   region   state  individuals  family_members  state_pop\n#> 1           1  Pacific  Alaska       1434.0           582.0     735139homelessness[homelessness[\"region\"].isin([\"South Atlantic\", \"Mid_atlantic\"])]\n#>     Unnamed: 0          region  ... family_members  state_pop\n#> 7            7  South Atlantic  ...          374.0     965479\n#> 8            8  South Atlantic  ...         3134.0     701547\n#> 9            9  South Atlantic  ...         9587.0   21244317\n#> 10          10  South Atlantic  ...         2556.0   10511131\n#> 20          20  South Atlantic  ...         2230.0    6035802\n#> 33          33  South Atlantic  ...         2817.0   10381615\n#> 40          40  South Atlantic  ...          851.0    5084156\n#> 46          46  South Atlantic  ...         2047.0    8501286\n#> 48          48  South Atlantic  ...          222.0    1804291\n#> \n#> [9 rows x 6 columns]"},{"path":"pandas-basics.html","id":"arrange-1","chapter":"7 Pandas basics","heading":"7.3.3 arrange","text":"我想看individuals的top5我想先依照region由小到大排，然後，region平手的，再依照family_members由大到小排，最後print出前五個","code":"homelessness.sort_values(\"individuals\", ascending = False).head()\n#>     Unnamed: 0              region  ... family_members  state_pop\n#> 4            4             Pacific  ...        20964.0   39461588\n#> 32          32        Mid-Atlantic  ...        52070.0   19530351\n#> 9            9      South Atlantic  ...         9587.0   21244317\n#> 43          43  West South Central  ...         6111.0   28628666\n#> 47          47             Pacific  ...         5880.0    7523869\n#> \n#> [5 rows x 6 columns]homelessness.sort_values([\"region\", \"family_members\"], ascending = [True, False]).head()\n#>     Unnamed: 0              region  ... family_members  state_pop\n#> 13          13  East North Central  ...         3891.0   12723071\n#> 35          35  East North Central  ...         3320.0   11676341\n#> 22          22  East North Central  ...         3142.0    9984072\n#> 49          49  East North Central  ...         2167.0    5807406\n#> 14          14  East North Central  ...         1482.0    6695497\n#> \n#> [5 rows x 6 columns]"},{"path":"pandas-basics.html","id":"mutate-and-rename-1","chapter":"7 Pandas basics","heading":"7.3.4 mutate and rename","text":"我想新增一個total的欄位，定義為individuals + family_members我想把new這個table的”region”改名為region2就好","code":"new = homelessness.assign(total = homelessness[\"individuals\"]+homelessness[\"family_members\"])\n\nnew[[\"individuals\", \"family_members\", \"total\"]].head()\n#>    individuals  family_members     total\n#> 0       2570.0           864.0    3434.0\n#> 1       1434.0           582.0    2016.0\n#> 2       7259.0          2606.0    9865.0\n#> 3       2280.0           432.0    2712.0\n#> 4     109008.0         20964.0  129972.0new.rename(columns = {\"region\": \"region2\"}).head()\n#>    Unnamed: 0             region2  ... state_pop     total\n#> 0           0  East South Central  ...   4887681    3434.0\n#> 1           1             Pacific  ...    735139    2016.0\n#> 2           2            Mountain  ...   7158024    9865.0\n#> 3           3  West South Central  ...   3009733    2712.0\n#> 4           4             Pacific  ...  39461588  129972.0\n#> \n#> [5 rows x 7 columns]"},{"path":"pandas-basics.html","id":"summarise","chapter":"7 Pandas basics","heading":"7.3.5 summarise","text":"接下來，先讀一個新的資料集看一下內容：看一下overall:算出”weekly_sales”的mean先定義四分位差，然後去計算temperature_c, fuel_price_usd_per_l, unemployment的四分位差","code":"sales = pd.read_csv(\"data/sales_subset.csv\")\nsales.shape\n#> (10774, 10)sales.head()\n#>    Unnamed: 0  store type  ...  temperature_c fuel_price_usd_per_l  unemployment\n#> 0           0      1    A  ...       5.727778             0.679451         8.106\n#> 1           1      1    A  ...       8.055556             0.693452         8.106\n#> 2           2      1    A  ...      16.816667             0.718284         7.808\n#> 3           3      1    A  ...      22.527778             0.748928         7.808\n#> 4           4      1    A  ...      27.050000             0.714586         7.808\n#> \n#> [5 rows x 10 columns]sales.info()\n#> <class 'pandas.core.frame.DataFrame'>\n#> RangeIndex: 10774 entries, 0 to 10773\n#> Data columns (total 10 columns):\n#>  #   Column                Non-Null Count  Dtype  \n#> ---  ------                --------------  -----  \n#>  0   Unnamed: 0            10774 non-null  int64  \n#>  1   store                 10774 non-null  int64  \n#>  2   type                  10774 non-null  object \n#>  3   department            10774 non-null  int64  \n#>  4   date                  10774 non-null  object \n#>  5   weekly_sales          10774 non-null  float64\n#>  6   is_holiday            10774 non-null  bool   \n#>  7   temperature_c         10774 non-null  float64\n#>  8   fuel_price_usd_per_l  10774 non-null  float64\n#>  9   unemployment          10774 non-null  float64\n#> dtypes: bool(1), float64(4), int64(3), object(2)\n#> memory usage: 768.2+ KBsales[\"weekly_sales\"].mean()\n#> 23843.95014850566import numpy as np\ndef iqr(x):\n  return x.quantile(0.75) - x.quantile(0.25)\n\nsales[[\"temperature_c\", \"fuel_price_usd_per_l\", \"unemployment\"]].agg([iqr, np.mean, np.min, np.max])\n#>       temperature_c  fuel_price_usd_per_l  unemployment\n#> iqr       16.583333              0.073176      0.565000\n#> mean      15.731978              0.749746      8.082009\n#> amin      -8.366667              0.664129      3.879000\n#> amax      33.827778              1.107674      9.765000"},{"path":"pandas-basics.html","id":"group_by-summarise-2","chapter":"7 Pandas basics","heading":"7.3.6 group_by + summarise","text":"我想先group_by “type(, B, C)”，再計算weekly_sales的sum我想by “type”+“is_holiday”，去算出weekly_sales的sum, min, max","code":"tt_series = sales.groupby(\"type\")[\"weekly_sales\"].sum()\nprint(tt_series)\n#> type\n#> A    2.337163e+08\n#> B    2.317840e+07\n#> Name: weekly_sales, dtype: float64\nprint(type(tt_series))\n#> <class 'pandas.core.series.Series'>tt_df = sales.groupby(\"type\")[[\"weekly_sales\"]].sum()\nprint(tt_df)\n#>       weekly_sales\n#> type              \n#> A     2.337163e+08\n#> B     2.317840e+07\nprint(type(tt_df))\n#> <class 'pandas.core.frame.DataFrame'>tt_group = sales.groupby([\"type\", \"is_holiday\"])[\"weekly_sales\"].agg([np.min, np.max])\nprint(tt_group)\n#>                     amin       amax\n#> type is_holiday                    \n#> A    False      -1098.00  293966.05\n#>      True        -598.00    5350.00\n#> B    False       -798.00  232558.51\n#>      True          31.41    1590.00\nprint(type(tt_group))\n#> <class 'pandas.core.frame.DataFrame'>"},{"path":"pandas-basics.html","id":"slice-by-.loc列名-行名-and-.iloc列數-行數","chapter":"7 Pandas basics","heading":"7.4 slice by .loc[列名, 行名] and .iloc[列數, 行數]","text":"loc iloc是pandas的method，所以是用data.loc[列名,行名], data.iloc[列index,行index]來運作錯誤: df.loc[[列名1, 列名2], :]正確: df.loc[[列名1, 列名2]]df.iloc[0,2] # 返回valuedf.iloc[[0],[2]] # 返回df typedf.iloc[[0,2], [0,1]] #多列多行df.iloc[[0,2]] # 多列df.iloc[:, [0,1]] # 多行這邊先讀個檔，複習這兩種用法：可以看到最左邊有rownames, 每個column有colnames","code":"cars = pd.read_csv(\"data/cars.csv\", index_col=0)\ncars\n#>      cars_per_cap        country  drives_right\n#> US            809  United States          True\n#> AUS           731      Australia         False\n#> JAP           588          Japan         False\n#> IN             18          India         False\n#> RU            200         Russia          True\n#> MOR            70        Morocco          True\n#> EG             45          Egypt          True"},{"path":"pandas-basics.html","id":"取某列某行得到value-非df-type","chapter":"7 Pandas basics","heading":"7.4.1 取某列某行(得到value, 非df type)","text":"抓出列名是JAP, 行名是drives_right的資料此時的結果，都會幫你reduce到最小的資料結構，這邊是boolean。如果我想保留dataframe的格式，那我就都要加中括號","code":"print(cars.loc[\"JAP\",\"drives_right\"])\n#> Falseprint(cars.iloc[2,2])\n#> False"},{"path":"pandas-basics.html","id":"取某列某行得到df-type","chapter":"7 Pandas basics","heading":"7.4.2 取某列某行(得到df type)","text":"","code":"print(cars.loc[[\"JAP\"],[\"drives_right\"]])\n#>      drives_right\n#> JAP         Falseprint(cars.iloc[[2],[2]])\n#>      drives_right\n#> JAP         False"},{"path":"pandas-basics.html","id":"取多列多行","chapter":"7 Pandas basics","heading":"7.4.3 取多列多行","text":"抓出列名是RU, MOR，行名是drives_right的資料","code":"print(cars.loc[[\"RU\", \"MOR\"],[\"drives_right\"]])\n#>      drives_right\n#> RU           True\n#> MOR          Trueprint(cars.iloc[[4, 5],[2]])\n#>      drives_right\n#> RU           True\n#> MOR          True"},{"path":"pandas-basics.html","id":"取多個列就好行我全要","chapter":"7 Pandas basics","heading":"7.4.4 取多個列就好，行我全要","text":"取US, JAP, RU這三個列要注意，不要寫成cars.loc[[\"US\", \"JAP\", \"RU\"], :]","code":"cars.loc[[\"US\", \"JAP\", \"RU\"]]\n#>      cars_per_cap        country  drives_right\n#> US            809  United States          True\n#> JAP           588          Japan         False\n#> RU            200         Russia          True"},{"path":"pandas-basics.html","id":"取多個行就好列我全要","chapter":"7 Pandas basics","heading":"7.4.5 取多個行就好，列我全要","text":"取cars_per_cap和drives_right要注意，不要寫成cars.loc[[\"cars_per_cap\",\"drives_right\"]]","code":"cars.loc[:,[\"cars_per_cap\",\"drives_right\"]]\n#>      cars_per_cap  drives_right\n#> US            809          True\n#> AUS           731         False\n#> JAP           588         False\n#> IN             18         False\n#> RU            200          True\n#> MOR            70          True\n#> EG             45          True"},{"path":"pandas-basics.html","id":"indexing","chapter":"7 Pandas basics","heading":"7.5 Indexing","text":"簡單講，就是把某個column，設成row index，那接下來就可以用.loc[[列名1, 列名2]]來進行filter更特別的是，row index可以接受multi-column，所以當我們想篩選出類似實驗設計的多種treatment時(例如”=1且B=2” 或 “=2且B=3”)，特別適合用這種方式來篩選。錯誤: df.loc[[“列名2”:“列名7”]]正確: df.loc[“列名2”:“列名7”]錯誤: df.loc[:, [“行名2”:“行名7”]]正確: df.loc[:, “行名2”:“行名7”]讀個新的檔(“temperatures.csv”)來玩玩看:看起來就是該country的city，在此date下的avg_temp_c至於第一行的Unnamed: 0，就是index而已，原本在csv中應該就是colname為空白看一下info:ok，不意外，date又是字串而已，簡單幫他轉一下：","code":"# set a column as index\ndf_ind = df.set_index(\"grade\")\n# remove an index\ndf_orig = df_ind.reset_index()df_ind = df.set_index(\"grade\")\ndf_ind.loc[\"1\"]df_ind = df.set_index(\"grade\")\ndf_ind.loc[[\"1\", \"3\"]]df_ind = df.set_index(\"grade\",\"type\")\n# subset inner level with tuple\ndf_ind.loc[(\"1\", \"A\"), (\"3\",\"C\")]\n# subset only outer level with list\ndf_ind.loc[[\"1\",\"3\"]]df_ind = df.set_index(\"col1\")\ndf_ind.sort_index()df_ind = df.set_index(\"col1\")\ndf_ind.sort_index(ascending = False)df_ind = df.set_index(\"col1\", \"col2\")\ndf_ind.sort_index(\n  level = [\"col1\",\"col2\"],\n  ascending = [True, False]\n)temperatures = pd.read_csv(\"data/temperatures.csv\")\ntemperatures.head()\n#>    Unnamed: 0        date     city        country  avg_temp_c\n#> 0           0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n#> 1           1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n#> 2           2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n#> 3           3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n#> 4           4  2000-05-01  Abidjan  Côte D'Ivoire      27.547temperatures = temperatures.drop(columns=['Unnamed: 0'])\ntemperatures.head()\n#>          date     city        country  avg_temp_c\n#> 0  2000-01-01  Abidjan  Côte D'Ivoire      27.293\n#> 1  2000-02-01  Abidjan  Côte D'Ivoire      27.685\n#> 2  2000-03-01  Abidjan  Côte D'Ivoire      29.061\n#> 3  2000-04-01  Abidjan  Côte D'Ivoire      28.162\n#> 4  2000-05-01  Abidjan  Côte D'Ivoire      27.547temperatures.info()\n#> <class 'pandas.core.frame.DataFrame'>\n#> RangeIndex: 16500 entries, 0 to 16499\n#> Data columns (total 4 columns):\n#>  #   Column      Non-Null Count  Dtype  \n#> ---  ------      --------------  -----  \n#>  0   date        16500 non-null  object \n#>  1   city        16500 non-null  object \n#>  2   country     16500 non-null  object \n#>  3   avg_temp_c  16407 non-null  float64\n#> dtypes: float64(1), object(3)\n#> memory usage: 515.8+ KBtemperatures[\"date\"] = pd.to_datetime(temperatures[\"date\"], format = \"%Y-%m-%d\")\ntemperatures.info()\n#> <class 'pandas.core.frame.DataFrame'>\n#> RangeIndex: 16500 entries, 0 to 16499\n#> Data columns (total 4 columns):\n#>  #   Column      Non-Null Count  Dtype         \n#> ---  ------      --------------  -----         \n#>  0   date        16500 non-null  datetime64[ns]\n#>  1   city        16500 non-null  object        \n#>  2   country     16500 non-null  object        \n#>  3   avg_temp_c  16407 non-null  float64       \n#> dtypes: datetime64[ns](1), float64(1), object(2)\n#> memory usage: 515.8+ KB"},{"path":"pandas-basics.html","id":"將city欄位設為index","chapter":"7 Pandas basics","heading":"7.5.1 將city欄位設為index","text":"nice，可以看到city被移到index的位子(也就是row_name)如果想回復原狀，就用.reset_index()","code":"temperatures_ind = temperatures.set_index(\"city\")\nprint(temperatures_ind.head())\n#>               date        country  avg_temp_c\n#> city                                         \n#> Abidjan 2000-01-01  Côte D'Ivoire      27.293\n#> Abidjan 2000-02-01  Côte D'Ivoire      27.685\n#> Abidjan 2000-03-01  Côte D'Ivoire      29.061\n#> Abidjan 2000-04-01  Côte D'Ivoire      28.162\n#> Abidjan 2000-05-01  Côte D'Ivoire      27.547temperatures_ind.reset_index().head()\n#>       city       date        country  avg_temp_c\n#> 0  Abidjan 2000-01-01  Côte D'Ivoire      27.293\n#> 1  Abidjan 2000-02-01  Côte D'Ivoire      27.685\n#> 2  Abidjan 2000-03-01  Côte D'Ivoire      29.061\n#> 3  Abidjan 2000-04-01  Côte D'Ivoire      28.162\n#> 4  Abidjan 2000-05-01  Côte D'Ivoire      27.547"},{"path":"pandas-basics.html","id":"filter出citymoscow或saint-petersburg","chapter":"7 Pandas basics","heading":"7.5.2 filter出city=“Moscow”或”Saint Petersburg”","text":"對比於之前的寫法：可以很明顯的看到，他的優點就是比原本pandas的filter法簡潔可以很明顯的看到，他的優點就是比原本pandas的filter法簡潔但缺點是：index現在變成是data了，而且會duplicate，所以違反了”tidy data” principles，看你介不介意拉。\n### filter出treatment型的資料但缺點是：index現在變成是data了，而且會duplicate，所以違反了”tidy data” principles，看你介不介意拉。\n### filter出treatment型的資料接著進階一點，我想filter出country-city pair的資料(就像實驗設計時，我想filter出一種treatment的資料一樣)接著進階一點，我想filter出country-city pair的資料(就像實驗設計時，我想filter出一種treatment的資料一樣)舉例來說，我想filter出country = “Brazil”且city = “Rio De Janeiro”的資料，或country = “Pakistan”且city = “Lahore”的資料舉例來說，我想filter出country = “Brazil”且city = “Rio De Janeiro”的資料，或country = “Pakistan”且city = “Lahore”的資料接著用.loc[]加上tuple來篩選","code":"temperatures_ind.loc[[\"Moscow\", \"Saint Petersburg\"]].head()\n#>              date country  avg_temp_c\n#> city                                 \n#> Moscow 2000-01-01  Russia      -7.313\n#> Moscow 2000-02-01  Russia      -3.551\n#> Moscow 2000-03-01  Russia      -1.661\n#> Moscow 2000-04-01  Russia      10.096\n#> Moscow 2000-05-01  Russia      10.357temperatures[temperatures[\"city\"].isin([\"Moscow\", \"Saint Petersburg\"])].head()\n#>             date    city country  avg_temp_c\n#> 10725 2000-01-01  Moscow  Russia      -7.313\n#> 10726 2000-02-01  Moscow  Russia      -3.551\n#> 10727 2000-03-01  Moscow  Russia      -1.661\n#> 10728 2000-04-01  Moscow  Russia      10.096\n#> 10729 2000-05-01  Moscow  Russia      10.357# 先把這兩個column都設為index\ntemperatures_ind = temperatures.set_index([\"country\", \"city\"])\ntemperatures_ind.head()\n#>                             date  avg_temp_c\n#> country       city                          \n#> Côte D'Ivoire Abidjan 2000-01-01      27.293\n#>               Abidjan 2000-02-01      27.685\n#>               Abidjan 2000-03-01      29.061\n#>               Abidjan 2000-04-01      28.162\n#>               Abidjan 2000-05-01      27.547rows_to_keep = [(\"Brazil\", \"Rio De Janeiro\"), (\"Pakistan\",\"Lahore\")]\ntemperatures_ind.loc[rows_to_keep].head()\n#>                              date  avg_temp_c\n#> country city                                 \n#> Brazil  Rio De Janeiro 2000-01-01      25.974\n#>         Rio De Janeiro 2000-02-01      26.699\n#>         Rio De Janeiro 2000-03-01      26.270\n#>         Rio De Janeiro 2000-04-01      25.750\n#>         Rio De Janeiro 2000-05-01      24.356"},{"path":"pandas-basics.html","id":"用.sort_index做排序","chapter":"7 Pandas basics","heading":"7.5.3 用.sort_index()做排序","text":"接下來，做一下arrange我想直接照index做排序(所以照預設，就是先ascending country，平手時，再ascending city)我也可以先照city做descending排序，平手時，再照country做ascending排序","code":"print(temperatures_ind.sort_index())\n#>                          date  avg_temp_c\n#> country     city                         \n#> Afghanistan Kabul  2000-01-01       3.326\n#>             Kabul  2000-02-01       3.454\n#>             Kabul  2000-03-01       9.612\n#>             Kabul  2000-04-01      17.925\n#>             Kabul  2000-05-01      24.658\n#> ...                       ...         ...\n#> Zimbabwe    Harare 2013-05-01      18.298\n#>             Harare 2013-06-01      17.020\n#>             Harare 2013-07-01      16.299\n#>             Harare 2013-08-01      19.232\n#>             Harare 2013-09-01         NaN\n#> \n#> [16500 rows x 2 columns]temperatures_ind.sort_index(level = [\"city\", \"country\"], ascending = [False, True])\n#>                             date  avg_temp_c\n#> country       city                          \n#> China         Xian    2000-01-01      -2.819\n#>               Xian    2000-02-01       0.943\n#>               Xian    2000-03-01       8.997\n#>               Xian    2000-04-01      13.714\n#>               Xian    2000-05-01      20.568\n#> ...                          ...         ...\n#> Côte D'Ivoire Abidjan 2013-05-01      27.652\n#>               Abidjan 2013-06-01      26.157\n#>               Abidjan 2013-07-01      24.951\n#>               Abidjan 2013-08-01      24.541\n#>               Abidjan 2013-09-01         NaN\n#> \n#> [16500 rows x 2 columns]"},{"path":"pandas-basics.html","id":"排序後取連續的列outer列","chapter":"7 Pandas basics","heading":"7.5.4 排序後，取連續的列(outer列)","text":"注意，不要寫成temperatures_srt.loc[[\"Pakistan\":\"Russia\"]]","code":"# Sort the index of temperatures_ind\ntemperatures_srt = temperatures_ind.sort_index()\n\n# Subset rows from Pakistan to Russia\nprint(temperatures_srt.loc[\"Pakistan\":\"Russia\"])\n#>                                 date  avg_temp_c\n#> country  city                                   \n#> Pakistan Faisalabad       2000-01-01      12.792\n#>          Faisalabad       2000-02-01      14.339\n#>          Faisalabad       2000-03-01      20.309\n#>          Faisalabad       2000-04-01      29.072\n#>          Faisalabad       2000-05-01      34.845\n#> ...                              ...         ...\n#> Russia   Saint Petersburg 2013-05-01      12.355\n#>          Saint Petersburg 2013-06-01      17.185\n#>          Saint Petersburg 2013-07-01      17.234\n#>          Saint Petersburg 2013-08-01      17.153\n#>          Saint Petersburg 2013-09-01         NaN\n#> \n#> [1155 rows x 2 columns]"},{"path":"pandas-basics.html","id":"排序後取連續的列inner列","chapter":"7 Pandas basics","heading":"7.5.5 排序後，取連續的列(inner列)","text":"注意，如果只寫inner level，例如寫成這樣： temperatures_srt.loc[\"Lahore\":\"Moscow\"]，那會回給你一個空白的df","code":"# Try to subset rows from Lahore to Moscow\nprint(temperatures_srt.loc[\"Lahore\":\"Moscow\"])\n\n# Subset rows from Pakistan, Lahore to Russia, Moscow\n#>                          date  avg_temp_c\n#> country city                             \n#> Mexico  Mexico     2000-01-01      12.694\n#>         Mexico     2000-02-01      14.677\n#>         Mexico     2000-03-01      17.376\n#>         Mexico     2000-04-01      18.294\n#>         Mexico     2000-05-01      18.562\n#> ...                       ...         ...\n#> Morocco Casablanca 2013-05-01      19.217\n#>         Casablanca 2013-06-01      23.649\n#>         Casablanca 2013-07-01      27.488\n#>         Casablanca 2013-08-01      27.952\n#>         Casablanca 2013-09-01         NaN\n#> \n#> [330 rows x 2 columns]\nprint(temperatures_srt.loc[(\"Pakistan\", \"Lahore\"):(\"Russia\", \"Moscow\")])\n#>                       date  avg_temp_c\n#> country  city                         \n#> Pakistan Lahore 2000-01-01      12.792\n#>          Lahore 2000-02-01      14.339\n#>          Lahore 2000-03-01      20.309\n#>          Lahore 2000-04-01      29.072\n#>          Lahore 2000-05-01      34.845\n#> ...                    ...         ...\n#> Russia   Moscow 2013-05-01      16.152\n#>          Moscow 2013-06-01      18.718\n#>          Moscow 2013-07-01      18.136\n#>          Moscow 2013-08-01      17.485\n#>          Moscow 2013-09-01         NaN\n#> \n#> [660 rows x 2 columns]"},{"path":"pandas-basics.html","id":"排序後取時間性資料","chapter":"7 Pandas basics","heading":"7.5.6 排序後，取時間性資料","text":"如果今天想subset出date的資料，例如我想找出2010~2011的資料就好，我會怎麼做？照之前的做法會這樣：那如果用現在的招，就把date移去當index，那就變成：good，多個練習，想看2010-08~2011-02的資料：","code":"# Use Boolean conditions to subset temperatures for rows in 2010 and 2011\ntemperatures_bool = temperatures[(temperatures[\"date\"] >= \"2010-01-01\") & (temperatures[\"date\"] <= \"2011-12-31\")]\nprint(temperatures_bool)\n#>             date     city        country  avg_temp_c\n#> 120   2010-01-01  Abidjan  Côte D'Ivoire      28.270\n#> 121   2010-02-01  Abidjan  Côte D'Ivoire      29.262\n#> 122   2010-03-01  Abidjan  Côte D'Ivoire      29.596\n#> 123   2010-04-01  Abidjan  Côte D'Ivoire      29.068\n#> 124   2010-05-01  Abidjan  Côte D'Ivoire      28.258\n#> ...          ...      ...            ...         ...\n#> 16474 2011-08-01     Xian          China      23.069\n#> 16475 2011-09-01     Xian          China      16.775\n#> 16476 2011-10-01     Xian          China      12.587\n#> 16477 2011-11-01     Xian          China       7.543\n#> 16478 2011-12-01     Xian          China      -0.490\n#> \n#> [2400 rows x 4 columns]# Set date as an index and sort the index\ntemperatures_ind = temperatures.set_index(\"date\").sort_index()\n\n# Use .loc[] to subset temperatures_ind for rows in 2010 and 2011\nprint(temperatures_ind.loc[\"2010\":\"2011\"])\n#>                   city    country  avg_temp_c\n#> date                                         \n#> 2010-01-01  Faisalabad   Pakistan      11.810\n#> 2010-01-01   Melbourne  Australia      20.016\n#> 2010-01-01   Chongqing      China       7.921\n#> 2010-01-01   São Paulo     Brazil      23.738\n#> 2010-01-01   Guangzhou      China      14.136\n#> ...                ...        ...         ...\n#> 2011-12-01      Nagoya      Japan       6.476\n#> 2011-12-01   Hyderabad      India      23.613\n#> 2011-12-01        Cali   Colombia      21.559\n#> 2011-12-01        Lima       Peru      18.293\n#> 2011-12-01     Bangkok   Thailand      25.021\n#> \n#> [2400 rows x 3 columns]# Use .loc[] to subset temperatures_ind for rows from Aug 2010 to Feb 2011\nprint(temperatures_ind.loc[\"2010-08\":\"2011-02\"])\n#>                 city        country  avg_temp_c\n#> date                                           \n#> 2010-08-01  Calcutta          India      30.226\n#> 2010-08-01      Pune          India      24.941\n#> 2010-08-01     Izmir         Turkey      28.352\n#> 2010-08-01   Tianjin          China      25.543\n#> 2010-08-01    Manila    Philippines      27.101\n#> ...              ...            ...         ...\n#> 2011-02-01     Kabul    Afghanistan       3.914\n#> 2011-02-01   Chicago  United States       0.276\n#> 2011-02-01    Aleppo          Syria       8.246\n#> 2011-02-01     Delhi          India      18.136\n#> 2011-02-01   Rangoon          Burma      26.631\n#> \n#> [700 rows x 3 columns]"},{"path":"pandas-basics.html","id":"subsetting-by-rowcolumn-number","chapter":"7 Pandas basics","heading":"7.5.7 subsetting by row/column number","text":"","code":"# Get the 23rd row, 2nd column (index positions 22 and 1).\nprint(temperatures.iloc[22,1])\n\n# Get the first 5 rows (index positions 0 to 5).\n#> Abidjan\nprint(temperatures.iloc[:5])\n\n# Get all rows, columns 3 and 4 (index positions 2 to 4).\n#>         date     city        country  avg_temp_c\n#> 0 2000-01-01  Abidjan  Côte D'Ivoire      27.293\n#> 1 2000-02-01  Abidjan  Côte D'Ivoire      27.685\n#> 2 2000-03-01  Abidjan  Côte D'Ivoire      29.061\n#> 3 2000-04-01  Abidjan  Côte D'Ivoire      28.162\n#> 4 2000-05-01  Abidjan  Côte D'Ivoire      27.547\nprint(temperatures.iloc[:,2:4])\n\n# Get the first 5 rows, columns 3 and 4.\n#>              country  avg_temp_c\n#> 0      Côte D'Ivoire      27.293\n#> 1      Côte D'Ivoire      27.685\n#> 2      Côte D'Ivoire      29.061\n#> 3      Côte D'Ivoire      28.162\n#> 4      Côte D'Ivoire      27.547\n#> ...              ...         ...\n#> 16495          China      18.979\n#> 16496          China      23.522\n#> 16497          China      25.251\n#> 16498          China      24.528\n#> 16499          China         NaN\n#> \n#> [16500 rows x 2 columns]\nprint(temperatures.iloc[:5, 2:4])\n#>          country  avg_temp_c\n#> 0  Côte D'Ivoire      27.293\n#> 1  Côte D'Ivoire      27.685\n#> 2  Côte D'Ivoire      29.061\n#> 3  Côte D'Ivoire      28.162\n#> 4  Côte D'Ivoire      27.547"},{"path":"pandas-basics.html","id":"pivot-table-1","chapter":"7 Pandas basics","heading":"7.6 pivot table","text":"","code":""},{"path":"pandas-basics.html","id":"intro","chapter":"7 Pandas basics","heading":"7.6.1 Intro","text":"pandas有個dplyr沒有的優點，就是他可以做excel的樞紐分析表最簡單的例子，就是你可以指定table的row是性別，col是年級，然後cell是去算該性別該年級下的平均成績之類的例如，我想by “type” + “is_holiday” 去算平均”weekly_sales”如果我想算marginal mean，那就：如果cross的地方，沒有數據，他的預設會是NA。例如：“department”+“type”去計算平均”weekly_sales”可以發現右下角出現NaN如果你想用數值來fill這個NaN，可以用fill_value =這個指令再來練習下一個題目，用”temperatures.csv”的資料我想看：各city/各年，的平均溫度而原始資料中，沒有”年”這個column，只有”date”這個column(“yyyy-mm-dd”)，所以我要再擷取出year的資訊","code":"df.pivot_table(\n  index = cat_col1, #row\n  columns = cat_col2, #col\n  values = con_col, #cell要by誰做計算\n  aggfunc = np.mean, #計算用的function\n  margins = True #是否要算overall row/col\n)sales.pivot_table(\n    index = \"type\", # pivot table的row要放啥\n    columns = \"is_holiday\", # pivot table的column要放啥\n    values = \"weekly_sales\", # pivot table 的 cell ，要對哪個變數做計算\n    aggfunc = np.mean # 計算的function要用甚麼\n)\n#> is_holiday         False       True\n#> type                               \n#> A           23768.583523  590.04525\n#> B           25751.980533  810.70500sales.pivot_table(\n    index = \"type\", # pivot table的row要放啥\n    columns = \"is_holiday\", # pivot table的column要放啥\n    values = \"weekly_sales\", # pivot table 的 cell ，要對哪個變數做計算\n    aggfunc = np.mean, # 計算的function要用甚麼\n    margins= True\n)\n#> is_holiday         False        True           All\n#> type                                              \n#> A           23768.583523  590.045250  23674.667242\n#> B           25751.980533  810.705000  25696.678370\n#> All         23934.913873  600.552857  23843.950149sales.pivot_table(\n  index = \"department\",\n  columns = \"type\",\n  values = \"weekly_sales\",\n  aggfunc= np.mean\n)\n#> type                    A              B\n#> department                              \n#> 1            30961.725379   44050.626667\n#> 2            67600.158788  112958.526667\n#> 3            17160.002955   30580.655000\n#> 4            44285.399091   51219.654167\n#> 5            34821.011364   63236.875000\n#> ...                   ...            ...\n#> 95          123933.787121   77082.102500\n#> 96           21367.042857    9528.538333\n#> 97           28471.266970    5828.873333\n#> 98           12875.423182     217.428333\n#> 99             379.123659            NaN\n#> \n#> [80 rows x 2 columns]sales.pivot_table(\n  index = \"department\",\n  columns = \"type\",\n  values = \"weekly_sales\",\n  aggfunc= np.mean,\n  fill_value = 0\n)\n#> type                    A              B\n#> department                              \n#> 1            30961.725379   44050.626667\n#> 2            67600.158788  112958.526667\n#> 3            17160.002955   30580.655000\n#> 4            44285.399091   51219.654167\n#> 5            34821.011364   63236.875000\n#> ...                   ...            ...\n#> 95          123933.787121   77082.102500\n#> 96           21367.042857    9528.538333\n#> 97           28471.266970    5828.873333\n#> 98           12875.423182     217.428333\n#> 99             379.123659       0.000000\n#> \n#> [80 rows x 2 columns]# Add a year column to temperatures\ntemperatures[\"year\"] = temperatures[\"date\"].dt.year\n\n# Pivot avg_temp_c by country and city vs year\ntemp_by_country_city_vs_year = temperatures.pivot_table(index = [\"country\",\"city\"], columns = \"year\", values = \"avg_temp_c\")\n\n# See the result\nprint(temp_by_country_city_vs_year)\n#> year                                 2000       2001  ...       2012       2013\n#> country       city                                    ...                      \n#> Afghanistan   Kabul             15.822667  15.847917  ...  14.510333  16.206125\n#> Angola        Luanda            24.410333  24.427083  ...  24.240083  24.553875\n#> Australia     Melbourne         14.320083  14.180000  ...  14.268667  14.741500\n#>               Sydney            17.567417  17.854500  ...  17.474333  18.089750\n#> Bangladesh    Dhaka             25.905250  25.931250  ...  26.283583  26.587000\n#> ...                                   ...        ...  ...        ...        ...\n#> United States Chicago           11.089667  11.703083  ...  12.821250  11.586889\n#>               Los Angeles       16.643333  16.466250  ...  17.089583  18.120667\n#>               New York           9.969083  10.931000  ...  11.971500  12.163889\n#> Vietnam       Ho Chi Minh City  27.588917  27.831750  ...  28.248750  28.455000\n#> Zimbabwe      Harare            20.283667  20.861000  ...  20.523333  19.756500\n#> \n#> [100 rows x 14 columns]"},{"path":"pandas-basics.html","id":"subsetting-pivot-table","chapter":"7 Pandas basics","heading":"7.6.2 subsetting pivot table","text":"那因為pivot table他就是一個pandas dataframe，所以你也可以用.loc去subset他，例如，我想subset出country從”Egypt”到”India”的資料我想subset出country-city從”Egypt-Cairo” 到 “India-Delhi”綜合行和列，subset出country-city從”Egypt-Cairo” 到 “India-Delhi”，以及年份從2005~2010的資料：","code":"# Subset for Egypt to India\ntemp_by_country_city_vs_year.loc[\"Egypt\":\"India\"]\n#> year                       2000       2001  ...       2012       2013\n#> country  city                               ...                      \n#> Egypt    Alexandria   20.744500  21.454583  ...  21.552583  21.438500\n#>          Cairo        21.486167  22.330833  ...  22.484250  22.907000\n#>          Gizeh        21.486167  22.330833  ...  22.484250  22.907000\n#> Ethiopia Addis Abeba  18.241250  18.296417  ...  18.448583  19.539000\n#> France   Paris        11.739667  11.371250  ...  11.219917  11.011625\n#> Germany  Berlin       10.963667   9.690250  ...   9.964333  10.121500\n#> India    Ahmadabad    27.436000  27.198083  ...  27.027250  27.608625\n#>          Bangalore    25.337917  25.528167  ...  26.042333  26.610500\n#>          Bombay       27.203667  27.243667  ...  27.192500  26.713000\n#>          Calcutta     26.491333  26.515167  ...  26.935083  27.369250\n#>          Delhi        26.048333  25.862917  ...  25.889417  26.709250\n#>          Hyderabad    27.231833  27.555167  ...  28.018583  28.851250\n#>          Jaipur       26.430250  26.023000  ...  25.884500  26.844125\n#>          Kanpur       25.353917  25.326500  ...  25.445417  26.121250\n#>          Lakhnau      25.353917  25.326500  ...  25.445417  26.121250\n#>          Madras       28.811667  29.162917  ...  29.778417  30.411750\n#>          Nagpur       26.181417  26.321667  ...  26.327917  27.112375\n#>          New Delhi    26.048333  25.862917  ...  25.889417  26.709250\n#>          Pune         25.110917  25.337833  ...  25.296833  25.847625\n#>          Surat        27.029000  26.897250  ...  26.889250  27.437750\n#> \n#> [20 rows x 14 columns]# Subset for Egypt, Cairo to India, Delhi\ntemp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\"):(\"India\",\"Delhi\")]\n#> year                       2000       2001  ...       2012       2013\n#> country  city                               ...                      \n#> Egypt    Cairo        21.486167  22.330833  ...  22.484250  22.907000\n#>          Gizeh        21.486167  22.330833  ...  22.484250  22.907000\n#> Ethiopia Addis Abeba  18.241250  18.296417  ...  18.448583  19.539000\n#> France   Paris        11.739667  11.371250  ...  11.219917  11.011625\n#> Germany  Berlin       10.963667   9.690250  ...   9.964333  10.121500\n#> India    Ahmadabad    27.436000  27.198083  ...  27.027250  27.608625\n#>          Bangalore    25.337917  25.528167  ...  26.042333  26.610500\n#>          Bombay       27.203667  27.243667  ...  27.192500  26.713000\n#>          Calcutta     26.491333  26.515167  ...  26.935083  27.369250\n#>          Delhi        26.048333  25.862917  ...  25.889417  26.709250\n#> \n#> [10 rows x 14 columns]# From Egypt, Cairo to India, Delhi, and 2005 to 2010.\ntemp_by_country_city_vs_year.loc[(\"Egypt\",\"Cairo\"):(\"India\",\"Delhi\"),\"2005\":\"2010\"]\n#> year                       2005       2006  ...       2009       2010\n#> country  city                               ...                      \n#> Egypt    Cairo        22.006500  22.050000  ...  22.625000  23.718250\n#>          Gizeh        22.006500  22.050000  ...  22.625000  23.718250\n#> Ethiopia Addis Abeba  18.312833  18.427083  ...  18.765333  18.298250\n#> France   Paris        11.552917  11.788500  ...  11.464083  10.409833\n#> Germany  Berlin        9.919083  10.545333  ...  10.062500   8.606833\n#> India    Ahmadabad    26.828083  27.282833  ...  28.095833  28.017833\n#>          Bangalore    25.476500  25.418250  ...  25.725750  25.705250\n#>          Bombay       27.035750  27.381500  ...  27.844500  27.765417\n#>          Calcutta     26.729167  26.986250  ...  27.153250  27.288833\n#>          Delhi        25.716083  26.365917  ...  26.554250  26.520250\n#> \n#> [10 rows x 6 columns]"},{"path":"pandas-basics.html","id":"calculating-on-a-pivot-table","chapter":"7 Pandas basics","heading":"7.6.3 Calculating on a pivot table","text":"因為pivot table還是pandas dataframe，所以你也可以對他做運算例如：這邊想做colmean和rowmean先看column mean，用的語法是 df.mean(axis = 0)例如，我們看看各年的溫度：挑出最大值的那年接著，看各city的溫度：找最低溫的city","code":"# Get the worldwide mean temp by year\nmean_temp_by_year = temp_by_country_city_vs_year.mean(axis = 0)\nprint(mean_temp_by_year)\n#> year\n#> 2000    19.506243\n#> 2001    19.679352\n#> 2002    19.855685\n#> 2003    19.630197\n#> 2004    19.672204\n#> 2005    19.607239\n#> 2006    19.793993\n#> 2007    19.854270\n#> 2008    19.608778\n#> 2009    19.833752\n#> 2010    19.911734\n#> 2011    19.549197\n#> 2012    19.668239\n#> 2013    20.312285\n#> dtype: float64# Filter for the year that had the highest mean temp\nprint(mean_temp_by_year[mean_temp_by_year == mean_temp_by_year.max()])\n#> year\n#> 2013    20.312285\n#> dtype: float64# Get the mean temp by city\nmean_temp_by_city = temp_by_country_city_vs_year.mean(axis = 1)\nprint(mean_temp_by_city)\n#> country        city            \n#> Afghanistan    Kabul               15.541955\n#> Angola         Luanda              24.391616\n#> Australia      Melbourne           14.275411\n#>                Sydney              17.799250\n#> Bangladesh     Dhaka               26.174440\n#>                                      ...    \n#> United States  Chicago             11.330825\n#>                Los Angeles         16.675399\n#>                New York            10.911034\n#> Vietnam        Ho Chi Minh City    27.922857\n#> Zimbabwe       Harare              20.699000\n#> Length: 100, dtype: float64# Filter for the city that had the lowest mean temp\nprint(mean_temp_by_city[mean_temp_by_city==mean_temp_by_city.min()])\n#> country  city  \n#> China    Harbin    4.876551\n#> dtype: float64"},{"path":"pandas-basics.html","id":"visualizing-dataframe","chapter":"7 Pandas basics","heading":"7.7 Visualizing DataFrame","text":"先讀一個資料集進來”avoplotto.pkl”avocado是酪梨，這筆資料是想了解酪梨在USA的供需狀況size是指酪梨的大小，分成三類(small, large, extra_large)type有兩種(organic, conventional)所以這筆資料，等於是看該類型該size的酪梨，在那個date下的賣價(avg_price)，以及賣出多少個(nb_sold = number sold)","code":"import pickle\nimport matplotlib.pyplot as pltwith open(\"data/avoplotto.pkl\", \"rb\") as file:\n  avocados = pickle.load(file)print(type(avocados))\n#> <class 'pandas.core.frame.DataFrame'>\navocados.head()\n#>          date          type  year  avg_price   size     nb_sold\n#> 0  2015-12-27  conventional  2015       0.95  small  9626901.09\n#> 1  2015-12-20  conventional  2015       0.98  small  8710021.76\n#> 2  2015-12-13  conventional  2015       0.93  small  9855053.66\n#> 3  2015-12-06  conventional  2015       0.89  small  9405464.36\n#> 4  2015-11-29  conventional  2015       0.99  small  8094803.56"},{"path":"pandas-basics.html","id":"bar","chapter":"7 Pandas basics","heading":"7.7.1 Bar","text":"我想了解，不同size，總計賣出多少個？別忘了，你summarise的方式是是對某個column做sum method，所以結果是series，不是dataframe!!畫成bar plot看起來，small是賣最好的，但和large沒有差太多，而extra_large則少非常多","code":"nb_sold_by_size = avocados.groupby(\"size\")[\"nb_sold\"].sum()\nprint(nb_sold_by_size)\n#> size\n#> extra_large    1.561752e+08\n#> large          2.015012e+09\n#> small          2.054936e+09\n#> Name: nb_sold, dtype: float64print(type(nb_sold_by_size))\n#> <class 'pandas.core.series.Series'>nb_sold_by_size.plot(kind = \"bar\", #不需要指定x軸是啥，y軸是啥，因為你現在只是個series\n                     title = \"hahaha\", \n                     rot = 45)\nplt.show()"},{"path":"pandas-basics.html","id":"line-plot","chapter":"7 Pandas basics","heading":"7.7.2 Line plot","text":"我想看一下，售出數量隨時間的變化別忘了，現在是series，不是dataframe畫趨勢圖吧：","code":"nb_sold_by_date = avocados.groupby(\"date\")[\"nb_sold\"].sum()\nprint(nb_sold_by_date.head())\n#> date\n#> 2015-01-04    27279606.03\n#> 2015-01-11    25081927.33\n#> 2015-01-18    24961540.48\n#> 2015-01-25    24094678.66\n#> 2015-02-01    39838734.08\n#> Name: nb_sold, dtype: float64print(type(nb_sold_by_date))\n#> <class 'pandas.core.series.Series'>nb_sold_by_date.plot(kind = \"line\", rot = 45)\nplt.show()"},{"path":"pandas-basics.html","id":"scatter-plot","chapter":"7 Pandas basics","heading":"7.7.3 scatter plot","text":"我想看看供需之間的關係，所以我想看賣出的數量(nb_sold)，與賣出的價錢(avg_price)的關係：看起來賣越多的時候，價格就降下來了","code":"avocados.plot(x = \"nb_sold\",\n              y = \"avg_price\",\n              kind = \"scatter\",\n              title = \"Number of avocados sold vs. average price\")\nplt.show()"},{"path":"pandas-basics.html","id":"histogram","chapter":"7 Pandas basics","heading":"7.7.4 Histogram","text":"","code":""},{"path":"pandas-basics.html","id":"single-histogram","chapter":"7 Pandas basics","heading":"7.7.4.1 single histogram","text":"我如果想看賣出價格的分布，我可以這樣做：","code":"avocados[\"avg_price\"].hist(bins = 20)\nplt.show()"},{"path":"pandas-basics.html","id":"multiple-histogram","chapter":"7 Pandas basics","heading":"7.7.4.2 multiple histogram","text":"如果我想比較兩個分布，例如，我想看conventional酪梨的價格分布，和organic酪梨的價格分布有啥不同：","code":"# Histogram of conventional avg_price \navocados[avocados[\"type\"] == \"conventional\"][\"avg_price\"].hist(bins = 20, alpha = 0.5)\n\n# Histogram of organic avg_price\navocados[avocados[\"type\"] == \"organic\"][\"avg_price\"].hist(bins = 20, alpha = 0.5)\n\n# Add a legend\nplt.legend([\"conventional\", \"organic\"])\n\n# Show the plot\nplt.show()"},{"path":"pandas-basics.html","id":"missing-values","chapter":"7 Pandas basics","heading":"7.8 Missing values","text":"先來做個假資料：可以看到有NA在裡面了","code":"sub_avocados = avocados.iloc[:4]\nsub_avocados.iloc[1,[3,5]] = float(\"NaN\")\n#> /Volumes/GoogleDrive/我的雲端硬碟/0. codepool_python/ds_tutorial/renv/python/virtualenvs/renv-python-3.8.0/lib/python3.8/site-packages/pandas/core/indexing.py:1817: SettingWithCopyWarning: \n#> A value is trying to be set on a copy of a slice from a DataFrame.\n#> Try using .loc[row_indexer,col_indexer] = value instead\n#> \n#> See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n#>   self._setitem_single_column(loc, value, pi)\nsub_avocados.iloc[[0,2],2] = float(\"NaN\")\nsub_avocados\n#>          date          type    year  avg_price   size     nb_sold\n#> 0  2015-12-27  conventional     NaN       0.95  small  9626901.09\n#> 1  2015-12-20  conventional  2015.0        NaN  small         NaN\n#> 2  2015-12-13  conventional     NaN       0.93  small  9855053.66\n#> 3  2015-12-06  conventional  2015.0       0.89  small  9405464.36"},{"path":"pandas-basics.html","id":"finding-missing-values","chapter":"7 Pandas basics","heading":"7.8.1 Finding missing values","text":"看dataframe哪個cell有NA看df各個column有沒有NA看df各個column的NA總數畫個bar chart看一下：","code":"sub_avocados.isna()\n#>     date   type   year  avg_price   size  nb_sold\n#> 0  False  False   True      False  False    False\n#> 1  False  False  False       True  False     True\n#> 2  False  False   True      False  False    False\n#> 3  False  False  False      False  False    Falsesub_avocados.isna().any()\n#> date         False\n#> type         False\n#> year          True\n#> avg_price     True\n#> size         False\n#> nb_sold       True\n#> dtype: boolsub_avocados.isna().sum()\n#> date         0\n#> type         0\n#> year         2\n#> avg_price    1\n#> size         0\n#> nb_sold      1\n#> dtype: int64sub_avocados.isna().sum().plot(kind = \"bar\")"},{"path":"pandas-basics.html","id":"remove-na","chapter":"7 Pandas basics","heading":"7.8.2 Remove NA","text":"如果要留下complete cases，可用pandas的.dropna()指令只剩這一個case了","code":"avocados_complete = sub_avocados.dropna()\nprint(avocados_complete)\n#>          date          type    year  avg_price   size     nb_sold\n#> 3  2015-12-06  conventional  2015.0       0.89  small  9405464.36"},{"path":"pandas-basics.html","id":"replacing-na","chapter":"7 Pandas basics","heading":"7.8.3 Replacing NA","text":"這邊只教了最簡單的，遺漏值全補0呵呵，很怪吧! 這只適用在，補0有意義的地方，例如，nb_sold如果是因為沒有賣出任何酪梨時，會寫NA，那這時補0就是對的詳細的處理missing value的方法，留待data cleaning的章節再來好好講","code":"avocados_fill_zero = sub_avocados.fillna(0)\nprint(avocados_fill_zero)\n#>          date          type    year  avg_price   size     nb_sold\n#> 0  2015-12-27  conventional     0.0       0.95  small  9626901.09\n#> 1  2015-12-20  conventional  2015.0       0.00  small        0.00\n#> 2  2015-12-13  conventional     0.0       0.93  small  9855053.66\n#> 3  2015-12-06  conventional  2015.0       0.89  small  9405464.36"},{"path":"join-data.html","id":"join-data","chapter":"8 Join data","heading":"8 Join data","text":"Chicago provides list taxicab owners vehicles licensed operate within city, public safety. goal merge two tables together.Chicago provides list taxicab owners vehicles licensed operate within city, public safety. goal merge two tables together.手邊現在有兩個table，一個是taxi_owners，裡面是計程車行的相關資訊，如下：手邊現在有兩個table，一個是taxi_owners，裡面是計程車行的相關資訊，如下：表中各欄位意義：\nrid: primary key，我猜是註冊的id，register id\nvid: 也可當primary key，\nowner: 車行名稱\naddress: 地址\nzip:\nrid: primary key，我猜是註冊的id，register idvid: 也可當primary key，owner: 車行名稱address: 地址zip:另一個是taxi_vehicles，是計程車的車子的相關資訊欄位說明如下：\nvid: primary key, 我猜是車牌吧\nmake: 哪個廠牌\nmodel: 哪個型號\nyear: 出廠年份\nfuel_type: 汽油、HYBRID、FLEX FUEL、COMPRESSED NATURAL GAS\nowner: 屬於哪家車行的車\nvid: primary key, 我猜是車牌吧make: 哪個廠牌model: 哪個型號year: 出廠年份fuel_type: 汽油、HYBRID、FLEX FUEL、COMPRESSED NATURAL GASowner: 屬於哪家車行的車","code":"import pandas as pdtaxi_owners = pd.read_pickle(r\"data/taxi_owners.p\")\ntaxi_owners.head()\n#>      rid   vid           owner                 address    zip\n#> 0  T6285  6285  AGEAN TAXI LLC     4536 N. ELSTON AVE.  60630\n#> 1  T4862  4862    MANGIB CORP.  5717 N. WASHTENAW AVE.  60659\n#> 2  T1495  1495   FUNRIDE, INC.     3351 W. ADDISON ST.  60618\n#> 3  T4231  4231    ALQUSH CORP.   6611 N. CAMPBELL AVE.  60645\n#> 4  T5971  5971  EUNIFFORD INC.     3351 W. ADDISON ST.  60618len(taxi_owners[\"vid\"].drop_duplicates())\n#> 3519taxi_owners.shape\n#> (3519, 5)taxi_vehicles = pd.read_pickle(r\"data/taxi_vehicles.p\")\ntaxi_vehicles.head()\n#>     vid    make   model  year fuel_type                owner\n#> 0  2767  TOYOTA   CAMRY  2013    HYBRID       SEYED M. BADRI\n#> 1  1411  TOYOTA    RAV4  2017    HYBRID          DESZY CORP.\n#> 2  6500  NISSAN  SENTRA  2019  GASOLINE       AGAPH CAB CORP\n#> 3  2746  TOYOTA   CAMRY  2013    HYBRID  MIDWEST CAB CO, INC\n#> 4  5922  TOYOTA   CAMRY  2013    HYBRID       SUMETTI CAB CO# Merge the taxi_owners and taxi_veh tables setting a suffix\ntaxi_own_veh = taxi_owners.merge(taxi_vehicles, on='vid', suffixes=[\"_own\", \"_veh\"])\n\n# Print the column names of taxi_own_veh\nprint(taxi_own_veh.columns)\n#> Index(['rid', 'vid', 'owner_own', 'address', 'zip', 'make', 'model', 'year',\n#>        'fuel_type', 'owner_veh'],\n#>       dtype='object')"},{"path":"join-data.html","id":"one-to-many","chapter":"8 Join data","heading":"8.1 One to many","text":"inner-join對one--many時，會重複量少的那邊","code":""},{"path":"join-data.html","id":"multiple-match","chapter":"8 Join data","heading":"8.2 multiple match","text":"","code":"df1.merge(df2, on = [\"col1\", \"col2\"])"},{"path":"join-data.html","id":"left-join","chapter":"8 Join data","heading":"8.3 left join","text":"","code":"df1.merge(df2, on = \"col1\", how = \"left\")"},{"path":"join-data.html","id":"left_id-match-right_id","chapter":"8 Join data","heading":"8.4 left_id match right_id","text":"","code":"df1.merge(df2, how = \"left\", left_on = \"left_id\", right_on = \"right_id\")"},{"path":"join-data.html","id":"outer-join-full-join","chapter":"8 Join data","heading":"8.5 outer join == full join","text":"##　自己join自己蠻有趣的，可以整理一下","code":"df1.merge(df2, how = \"outer\")"},{"path":"join-data.html","id":"mutating-joins","chapter":"8 Join data","heading":"8.6 mutating joins","text":"就是剛剛的inner join, full join, left join, right join，他join完後是會增加column的那種","code":""},{"path":"join-data.html","id":"filtering-join","chapter":"8 Join data","heading":"8.7 filtering join","text":"join的目的，是在filitering，所以join完後，不會增加column\nx semi-join y，就是 x left-join y後，只留x的column，和matching column != na的部分\nx anti-join y，就是 x left-join y後，只留x的column，和matching column == na的部分\nx semi-join y，就是 x left-join y後，只留x的column，和matching column != na的部分x anti-join y，就是 x left-join y後，只留x的column，和matching column == na的部分遺憾的是，pandas沒有這semi_join和anti_join的指令，你還真的得照上面的步驟，做出結果","code":""}]
